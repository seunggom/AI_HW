{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = np.loadtxt('mnist.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(csv_dataset):\n",
    "    '''\n",
    "    csv_dataset의 shape는 (10000, 785)이다.\n",
    "    총 만 개의 데이터가 있고, 각 데이터는 레이블값(1개), 픽셀값(784개)들로 이루어져 있다.\n",
    "    train_set 개수 : test_set 개수 = 80 : 20 의 비율로 데이터를 분할하고 레이블값과 픽셀값으로 한 번 더 분할하면,\n",
    "    train_X.shape = (8000, 784)\n",
    "    train_T.shape = (8000, 1)\n",
    "    test_X.shape = (2000, 784)\n",
    "    test_T.shape = (2000, 1) 이다.\n",
    "    '''\n",
    "    train_X = csv_dataset[:8000, 1:]\n",
    "    train_X /= 256\n",
    "    train_T = csv_dataset[:8000, 0]\n",
    "    test_X = csv_dataset[8000:, 1:]\n",
    "    test_X /= 256\n",
    "    test_T = csv_dataset[8000:, 0]  \n",
    "    \n",
    "    return train_X, train_T, test_X, test_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(T): # T is data의 label\n",
    "    one_hot_label = np.zeros([T.shape[0],10])\n",
    "    T = T.astype(np.uint8)\n",
    "    one_hot_label[np.arange(T.shape[0]), T] = 1\n",
    "    \n",
    "    '''\n",
    "    먼저 [데이터개수, 10] 크기의 배열을 만든다\n",
    "    각 데이터마다의 레이블값과 같은 인덱스열에 1의 값을 넣어주어야 하는데,\n",
    "    T에 들어있는 값은 float형이기 때문에, 형변환을 하지 않고 3번째 줄을 실행하면\n",
    "    IndexError: arrays used as indices must be of integer (or boolean) type 와 같은 에러가 발생한다.\n",
    "    그래서 np.astype을 이용해 int형으로 변환해주었다.\n",
    "    그리고 one_hot_label에서 각 행마다 정답 인덱스에 해당하는 열에 1의 값을 저장한다.\n",
    "    \n",
    "    이 함수의 예를 들자면, T=[7,2]일 때 one_hot_label은\n",
    "    [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
    "     [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 가 나오게 된다.\n",
    "    '''\n",
    "\n",
    "    return one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(ScoreMatrix): # 제공.\n",
    "\n",
    "    if ScoreMatrix.ndim == 2:\n",
    "        temp = ScoreMatrix\n",
    "        temp = temp - np.max(temp, axis=1, keepdims=True)\n",
    "        y_predict = np.exp(temp) / np.sum(np.exp(temp), axis=1, keepdims=True)\n",
    "        return y_predict\n",
    "    temp = ScoreMatrix - np.max(ScoreMatrix, axis=0)\n",
    "    expX = np.exp(temp)\n",
    "    y_predict = expX / np.sum(expX)\n",
    "    return y_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setParam_He(neuronlist):\n",
    "    \n",
    "    np.random.seed(1) # seed값 고정을 통해 input이 같으면 언제나 같은 Weight와 bias를 출력하기 위한 함수\n",
    "    \n",
    "    '''\n",
    "    input layer neuron, hidden layer1 neuron, hidden layer2 neuron과 연산을 할 각각의 Weight가 필요하다.\n",
    "    이 Weight값과 Bias 값을 He의 방법으로 초기화를 한다.\n",
    "    He의 방법을 이용한 Weight의 초기값이란 앞 계층의 노드가 n 개일 때, 표준편차가 (2/n)^0.5 인 정규분포를 사용하는 것을 말한다.\n",
    "    input layer neuron의 크기는 [데이터 개수, 784],\n",
    "    hidden layer1 neuron의 크기는 [데이터 개수, 60],\n",
    "    hidden layer2 neuron의 크기는 [데이터 개수, 30],\n",
    "    output layer neuron의 크기는 [데이터 개수, 10] 이므로\n",
    "    필요한 Weight의 크기는 순서대로 [784, 60], [60, 30], [30, 10]이 될 것이다.\n",
    "    bias의 경우에는 초기값으로 전부 0의 값을 제공한다.\n",
    "    bias의 크기는 순서대로 [60, ], [30, ], [10, ] 이다.\n",
    "    '''\n",
    "    \n",
    "    W1 = np.random.randn(neuronlist[0], neuronlist[1]) / np.sqrt(neuronlist[0]/2)\n",
    "    W2 = np.random.randn(neuronlist[1], neuronlist[2]) / np.sqrt(neuronlist[1]/2)\n",
    "    W3 = np.random.randn(neuronlist[2], neuronlist[3]) / np.sqrt(neuronlist[2]/2)\n",
    "    b1 = np.zeros(neuronlist[1])\n",
    "    b2 = np.zeros(neuronlist[2])\n",
    "    b3 = np.zeros(neuronlist[3])\n",
    "        \n",
    "    return W1, W2, W3, b1, b2, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearLayer:\n",
    "    '''\n",
    "    이 클래스의 인스턴스는 ThreeLayerNet 클래스의 __init__() 메서드에서 만들어진다.\n",
    "    각각의 레이어에 필요한 forward, backward 연산을 하는 함수를 제공한다.\n",
    "    '''\n",
    "    def __init__(self, W, b):\n",
    "        #backward에 필요한 X, W, b 값 저장 + dW, db값 받아오기\n",
    "        \n",
    "        self.X = None\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        내적연산을 통한 Z값 계산하는 함수이다.\n",
    "        Z가 의미하는 것은 이전 레이어의 뉴런들에 weight만큼의 가중치를 적용한 신호의 총합들이다.\n",
    "        즉 값이 높은 뉴런일 수록 그것이 정답일 가능성이 높다고 추측한 것이다.\n",
    "        '''\n",
    "        self.X = x\n",
    "        Z = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        #백워드 함수\n",
    "        '''\n",
    "        gradient를 계산하는 함수이다.\n",
    "        dx의 크기는 [데이터 개수, 뉴런 개수]x[뉴런 개수, 784] = [데이터 개수, 784]\n",
    "        dW의 크기는 [784, 데이터 개수]x[데이터 개수, 뉴런 개수] = [784, 뉴런 개수]\n",
    "        db의 크기는 [뉴런 개수, ] 이므로,\n",
    "        크기에 맞게 내적연산을 진행한다.\n",
    "        '''\n",
    "        dx = np.dot(dZ, self.W.T)\n",
    "        self.dW = np.dot(self.X.T, dZ)\n",
    "        self.db = np.sum(dZ, axis = 0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    '''\n",
    "    드랍아웃을 위한 클래스이다.\n",
    "    드랍아웃을 사용할 경우 이 클래스의 객체가 생성되고, 각 히든 레이어마다 kill_rate만큼 값을 죽인다.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.kill_rate = None\n",
    "        self.U = None\n",
    "\n",
    "    def forward(self, D, train_flag=True):\n",
    "        '''\n",
    "        드랍아웃으로 뉴런을 죽이는 것은 training 과정에서만 진행해야하므로\n",
    "        forward는 train_flag로 training과 testing 과정을 구분하여 진행한다.\n",
    "        U = np.random.rand(*D.shape) >= kill_rate 의 의미를 설명하자면,\n",
    "        np.random.rand(*D.shape)은 D의 크기만큼 0에서 1사이의 값을 가지는 행렬을 리턴한다.\n",
    "        이 때 kill_rate 이상인 값을 가지면 식이 참이므로 1의 값을 가지고 kill_rate 미만의 값을 가지면 식이 거짓이므로 0을 가진다.\n",
    "        그래서 0 혹은 1의 값을 가지고 있는 U을 x에 곱하면, kill_rate의 확률만큼 뉴런이 죽게된다.\n",
    "        \n",
    "        testing 과정에서는 모든 뉴런을 사용하기 때문에\n",
    "        training 과정에서의 출력 뉴런 데이터의 기대값과 동일한 기대값을 갖기 위해서는 (1 - kill_rate) 만큼 곱해주어야 한다.\n",
    "        '''\n",
    "        if train_flag:\n",
    "            self.U = np.random.rand(*D.shape) >= self.kill_rate\n",
    "            return D * self.U\n",
    "        else:\n",
    "            return D * (1 - self.kill_rate)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        '''\n",
    "        forward 과정에서 죽였던 뉴런 위치 그대로 backward를 진행한다.\n",
    "        '''\n",
    "        dx = dout * self.U\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiLU:\n",
    "    '''\n",
    "    SiLU란 f(z) = z ∗ sigmoid(z)로 나타나는 함수이다.\n",
    "    forward함수에서는, z라는 입력이 들어오면 SiLU를 activation function으로 하여 activate한 후 그 결과를 self.Z에 저장한다.\n",
    "    backward함수에서는, 저장한 Z값으로 SiLU의 미분값을 구한 후 앞의 레이어에서 backward로 들어온 dActivation 값을 곱한 값 dZ를 출력한다.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = None # 백워드 시 사용할 로컬 변수\n",
    "       \n",
    "    \n",
    "    def forward(self, Z):\n",
    "        #수식에 따른 forward 함수 작성\n",
    "        sig = 1 / (1 + np.exp(-Z))\n",
    "        Activation = Z * sig\n",
    "        self.Z = Z\n",
    "\n",
    "        return Activation\n",
    "    \n",
    "    \n",
    "    def backward(self, dActivation):\n",
    "        '''\n",
    "        연산 과정을 도식화하면 아래와 같다.\n",
    "          Z                    Activation\n",
    "        ----------> (SiLU) ---------------->\n",
    "          dZ                 dActivation\n",
    "\n",
    "        이 때, f'(z) = f(z) + sigmoid(z)(1-f(z)) 이므로\n",
    "        dZ = (f(z) + sigmoid(z)(1-f(z))) * dActivation 이다.\n",
    "        '''\n",
    "        sig = 1 / (1 + np.exp(-self.Z))\n",
    "        fz = self.forward(self.Z)\n",
    "        dZ = (sig * (1 - fz) + fz) * dActivation\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss(): # 제공\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.softmaxScore = None\n",
    "        self.label = None\n",
    "        \n",
    "    def forward(self, score, one_hot_label):\n",
    "        \n",
    "        batch_size = one_hot_label.shape[0]\n",
    "        self.label = one_hot_label\n",
    "        self.softmaxScore = Softmax(score)\n",
    "        self.loss = -np.sum(self.label * np.log(self.softmaxScore + 1e-20)) / batch_size\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.label.shape[0]\n",
    "        dx = (self.softmaxScore - self.label) / batch_size\n",
    "        \n",
    "        return dx\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNet :\n",
    "    \n",
    "    def __init__(self, paramlist):\n",
    "        \n",
    "        W1, W2, W3, b1, b2, b3 = setParam_He(paramlist)\n",
    "        self.params = {}\n",
    "        self.params['W1'] = W1\n",
    "        self.params['W2'] = W2\n",
    "        self.params['W3'] = W3\n",
    "        self.params['b1'] = b1\n",
    "        self.params['b2'] = b2\n",
    "        self.params['b3'] = b3\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        self.layers['L1'] = linearLayer(self.params['W1'], self.params['b1'])\n",
    "        self.layers['SiLU1'] = SiLU()\n",
    "        self.layers['L2'] = linearLayer(self.params['W2'], self.params['b2'])\n",
    "        self.layers['SiLU2'] = SiLU()\n",
    "        self.layers['L3'] = linearLayer(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "        \n",
    "    def scoreFunction(self, x):\n",
    "        '''\n",
    "        모든 레이어에 대해 차례대로 forward를 진행한다.\n",
    "        그리고 리턴값으로는 forward를 모두 마친 후의 score값을 리턴한다.\n",
    "        '''\n",
    "        for layer in self.layers.values():\n",
    "            # 한 줄이 best\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        score = x\n",
    "        return score\n",
    "        \n",
    "    def forward(self, x, label):\n",
    "        '''\n",
    "        위의 scoreFunction 함수를 이용해 score를 구하고,\n",
    "        loss를 구하여 리턴하는 함수이다.\n",
    "        '''\n",
    "        score = self.scoreFunction(x)\n",
    "        return self.lastLayer.forward(score, label)\n",
    "    \n",
    "    def accuracy(self, x, label):\n",
    "        \n",
    "        score = self.scoreFunction(x)\n",
    "        score_argmax = np.argmax(score, axis=1)\n",
    "        \n",
    "        if label.ndim != 1 : #label이 one_hot_encoding 된 데이터면 if문을 \n",
    "            label_argmax = np.argmax(label, axis = 1)\n",
    "            \n",
    "        accuracy = np.sum(score_argmax==label_argmax) / int(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def backpropagation(self):\n",
    "        '''\n",
    "        forward를 진행한 레이어 순서의 반대 순서로 backward를 진행한다.\n",
    "        backward 후, 각 레이어 내 저장되어 있는 dW, db값을 grads 라는 딕셔너리 객체에 저장한 후 그 값을 리턴한다.\n",
    "        \n",
    "        '''\n",
    "        dL = self.lastLayer.backward()\n",
    "        dA2 = self.layers['L3'].backward(dL)\n",
    "        dZ2 = self.layers['SiLU2'].backward(dA2)\n",
    "        dA1 = self.layers['L2'].backward(dZ2)\n",
    "        dZ1 = self.layers['SiLU1'].backward(dA1)\n",
    "        d = self.layers['L1'].backward(dZ1)\n",
    "            \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['L1'].dW\n",
    "        grads['b1'] = self.layers['L1'].db\n",
    "        grads['W2'] = self.layers['L2'].dW\n",
    "        grads['b2'] = self.layers['L2'].db\n",
    "        grads['W3'] = self.layers['L3'].dW\n",
    "        grads['b3'] = self.layers['L3'].db\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradientdescent(self, grads, learning_rate):\n",
    "        \n",
    "        self.params['W1'] -= learning_rate*grads['W1']\n",
    "        self.params['W2'] -= learning_rate*grads['W2']\n",
    "        self.params['W3'] -= learning_rate*grads['W3']\n",
    "        self.params['b1'] -= learning_rate*grads['b1']\n",
    "        self.params['b2'] -= learning_rate*grads['b2']\n",
    "        self.params['b3'] -= learning_rate*grads['b3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UseDropout_ThreeLayerNet:\n",
    "    '''\n",
    "    처음엔 기존의 ThreeLayerNet 클래스를 수정하여 드랍아웃 구현을 하려고 했는데,\n",
    "    구현할수록 코드가 복잡해져서 드랍아웃 과정이 추가된 ThreeLayerNet 클래스를 따로 구현하였다.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, paramlist):\n",
    "        \n",
    "        W1, W2, W3, b1, b2, b3 = setParam_He(paramlist)\n",
    "        self.params = {}\n",
    "        self.params['W1'] = W1\n",
    "        self.params['W2'] = W2\n",
    "        self.params['W3'] = W3\n",
    "        self.params['b1'] = b1\n",
    "        self.params['b2'] = b2\n",
    "        self.params['b3'] = b3\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    " \n",
    "        self.layers['L1'] = linearLayer(self.params['W1'], self.params['b1'])\n",
    "        self.layers['SiLU1'] = SiLU()\n",
    "        self.layers['L2'] = linearLayer(self.params['W2'], self.params['b2'])\n",
    "        self.layers['SiLU2'] = SiLU()\n",
    "        self.layers['L3'] = linearLayer(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        #드랍아웃을 위한 레이어 두개\n",
    "        self.dropout_layer1 = Dropout()\n",
    "        self.dropout_layer2 = Dropout()\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "        \n",
    "    def scoreFunction(self, x, train_flag):\n",
    "        '''\n",
    "        레이어 순서대로 forward를 진행하는데, 뉴런값들을 activate한 후에 train_flag에 따라서 dropout을 진행한다.\n",
    "        '''\n",
    "        x = self.layers['L1'].forward(x)\n",
    "        x = self.layers['SiLU1'].forward(x)\n",
    "        u1 = self.dropout_layer1.forward(x, train_flag)\n",
    "        x = self.layers['L2'].forward(u1)\n",
    "        x = self.layers['SiLU2'].forward(x)\n",
    "        u2 = self.dropout_layer2.forward(x, train_flag)\n",
    "        x = self.layers['L3'].forward(u2)\n",
    "        \n",
    "        score = x\n",
    "        return score\n",
    "        \n",
    "    def forward(self, x, label, train_flag=True):\n",
    "        '''\n",
    "        score를 계산하고, 그것에 따라 Loss를 리턴한다.\n",
    "        '''\n",
    "        score = self.scoreFunction(x, train_flag)\n",
    "\n",
    "        return self.lastLayer.forward(score, label)\n",
    "    \n",
    "    def accuracy(self, x, label):\n",
    "        '''\n",
    "        정확도를 구하는 함수이다.\n",
    "        이 때는 오로지 각 데이터셋에 대한 정확도를 구하면 되기 때문에 train_flag를 false로 설정하여 뉴런을 죽이는 과정은 하지 않는다.\n",
    "        '''\n",
    "        train_flag = False\n",
    "        score = self.scoreFunction(x, train_flag)\n",
    "        score_argmax = np.argmax(score, axis=1)\n",
    "        \n",
    "        if label.ndim != 1 : #label이 one_hot_encoding 된 데이터면 if문을 \n",
    "            label_argmax = np.argmax(label, axis = 1)\n",
    "            \n",
    "        accuracy = np.sum(score_argmax==label_argmax) / int(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def backward(self):\n",
    "        '''\n",
    "        dropout 레이어의 forward에서 지정한 확률만큼 뉴런을 죽인것처럼, backward에서도 똑같이 gradient값을 죽인다.\n",
    "        forward에서 진행한 레이어 순서의 반대로 진행한다.\n",
    "        '''\n",
    "        dL = self.lastLayer.backward()\n",
    "        dA2 = self.layers['L3'].backward(dL)\n",
    "        dD2 = self.dropout_layer2.backward(dA2)\n",
    "        dZ2 = self.layers['SiLU2'].backward(dD2)\n",
    "        dA1 = self.layers['L2'].backward(dZ2)\n",
    "        dD1 = self.dropout_layer1.backward(dA1)\n",
    "        dZ1 = self.layers['SiLU1'].backward(dD1)\n",
    "        d = self.layers['L1'].backward(dZ1)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['L1'].dW\n",
    "        grads['b1'] = self.layers['L1'].db\n",
    "        grads['W2'] = self.layers['L2'].dW\n",
    "        grads['b2'] = self.layers['L2'].db\n",
    "        grads['W3'] = self.layers['L3'].dW\n",
    "        grads['b3'] = self.layers['L3'].db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def gradientdescent(self, grads, learning_rate):\n",
    "        \n",
    "        self.params['W1'] -= learning_rate*grads['W1']\n",
    "        self.params['W2'] -= learning_rate*grads['W2']\n",
    "        self.params['W3'] -= learning_rate*grads['W3']\n",
    "        self.params['b1'] -= learning_rate*grads['b1']\n",
    "        self.params['b2'] -= learning_rate*grads['b2']\n",
    "        self.params['b3'] -= learning_rate*grads['b3']\n",
    "        \n",
    "        \n",
    "    def setKillRate(self, r1, r2):\n",
    "        '''\n",
    "        각 히든레이어에서 쓸 kill_rate의 값을 받아서 업데이트한다.\n",
    "        '''\n",
    "        self.dropout_layer1.kill_rate = r1\n",
    "        self.dropout_layer2.kill_rate = r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchOptimization(dataset, ThreeLayerNet, learning_rate, epoch=1000):\n",
    "    '''\n",
    "    이 함수에서는 한 epoch마다 8000개의 train_X를 한 번에 forward 해서 Loss를 구하고,\n",
    "    backpropagation, gradientdescent를 사용해 W와 b를 업데이트 한다.\n",
    "    그리고 10번째마다 loss와 정확도를 출력한다.\n",
    "    '''\n",
    "    \n",
    "    for i in range(epoch+1):\n",
    "        #코드 작성\n",
    "        train_acc_list = []\n",
    "        test_acc_list = []\n",
    "        Loss_list = []\n",
    "        # 위의 것들은 append를 할 때 필요한 것이다. append()는 기존 리스트의 뒤에 새로 추가하라는 의미이다.\n",
    "        \n",
    "        Loss = ThreeLayerNet.forward(dataset['train_X'], dataset['one_hot_train'])\n",
    "        grad = ThreeLayerNet.backpropagation()\n",
    "        ThreeLayerNet.gradientdescent(grad, learning_rate)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            train_acc = ThreeLayerNet.accuracy(dataset['train_X'], dataset['one_hot_train'])\n",
    "            test_acc = ThreeLayerNet.accuracy(dataset['test_X'], dataset['one_hot_test'])\n",
    "            print(i, '\\t번째 Loss = ', Loss)\n",
    "            print(i, '\\t번째 Train_Accuracy : ', train_acc)\n",
    "            print(i, '\\t번째 Test_Accuracy : ', test_acc)\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            Loss_list.append(Loss)\n",
    "   \n",
    "    return ThreeLayerNet, train_acc_list, test_acc_list, Loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_Optimization(dataset, ThreeLayerNet, learning_rate, epoch=100, batch_size=1000):\n",
    "    '''\n",
    "    이 함수에서는 minibatch로 나누어서 optimization을 진행한다.\n",
    "    먼저 train_X와 one_hot_train을 random하게 섞는데, 두 배열 간의 관계가 달라지면 안되기 때문에\n",
    "    np.concatenate()를 이용해 임시로 두 배열을 합치고 나서 np.random.shuffle()을 이용해 랜덤하게 섞는다.\n",
    "    그러고나서 다시 처음처럼 train_X와 one_hot_train을 분할한다.\n",
    "    한 epoch는 모든 minibatch들을 다 진행했을 때를 말한다.\n",
    "    그래서 미니배치 사이즈가 100이고 데이터의 갯수가 8000개라면 총 80번의 minibatch를 돌아야 1 epoch인 것이다.\n",
    "    '''\n",
    "\n",
    "    np.random.seed(5)\n",
    "    for i in range(epoch+1):\n",
    "        # 코드 작성\n",
    "        train_acc_list = []\n",
    "        test_acc_list = []\n",
    "        Loss_list = []\n",
    "\n",
    "        tmp_train = np.concatenate((dataset['one_hot_train'], dataset['train_X']), axis=1)\n",
    "        np.random.shuffle(tmp_train)\n",
    "        dataset['one_hot_train'] = tmp_train[:, :10]\n",
    "        dataset['train_X'] = tmp_train[:, 10:]\n",
    "\n",
    "        batch = {}\n",
    "        b = 0\n",
    "        '''\n",
    "        미니배치 크기가 100이고 데이터의 갯수가 8000개인 경우\n",
    "        위에서 랜덤하게 섞은 데이터를 매 반복문마다 슬라이싱을 이용해\n",
    "        0~99번째, 100~199번째, ..., 7900~7999번째 데이터로 나누어서 연산을 진행한다.\n",
    "        '''\n",
    "        while b < (tmp_train.shape[0] / batch_size):\n",
    "            batch['train_X'] = dataset['train_X'][(batch_size * b):(batch_size * (b+1)), :]\n",
    "            batch['one_hot_train'] = dataset['one_hot_train'][(batch_size * b):(batch_size * (b+1)), :]\n",
    "            \n",
    "            Loss = ThreeLayerNet.forward(batch['train_X'], batch['one_hot_train'])\n",
    "            grad = ThreeLayerNet.backpropagation()\n",
    "            ThreeLayerNet.gradientdescent(grad, learning_rate)\n",
    "            # 각각의 미니배치마다 loss, gradient를 구해서 weight와 bias를 업데이트 한다.\n",
    "            b += 1\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            train_acc = ThreeLayerNet.accuracy(dataset['train_X'], dataset['one_hot_train'])\n",
    "            test_acc = ThreeLayerNet.accuracy(dataset['test_X'], dataset['one_hot_test'])\n",
    "            print(i, '\\t번째 Loss = ', Loss)\n",
    "            print(i, '\\t번째 Train_Accuracy : ', train_acc)\n",
    "            print(i, '\\t번째 Test_Accuracy : ', test_acc)\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            Loss_list.append(Loss)  \n",
    "\n",
    "    return ThreeLayerNet, train_acc_list, test_acc_list, Loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_use_Optimizer(dataset, UseDropout_ThreeLayerNet, learning_rate, epoch, kill_n_h1 = 0.25, kill_n_h2 = 0.15):\n",
    "    '''\n",
    "    Dropout을 사용한 Optimization이다.\n",
    "    Dropout을 사용하는 이유는 train data에만 치중하여 weight를 업데이트하면\n",
    "    오히려 test data의 정확도가 떨어질 수 있어 이를 방지하기 위함이다.\n",
    "    kill_n_h1, kill_n_h2은 드롭아웃으로 뉴런을 죽이는 비율이다.\n",
    "    UseDropout_ThreeLayerNet 클래스를 이용하였다.\n",
    "    '''\n",
    "    \n",
    "    UseDropout_ThreeLayerNet.setKillRate(kill_n_h1, kill_n_h2) # 뉴런을 죽일 비율을 dropout 레이어에 전달한다.\n",
    "    \n",
    "    for i in range(epoch+1):\n",
    "        #코드 작성\n",
    "        train_acc_list = []\n",
    "        test_acc_list = []\n",
    "        Loss_list = []\n",
    "    \n",
    "        Loss = UseDropout_ThreeLayerNet.forward(dataset['train_X'], dataset['one_hot_train'])\n",
    "        grad = UseDropout_ThreeLayerNet.backward()\n",
    "        UseDropout_ThreeLayerNet.gradientdescent(grad, learning_rate)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            train_acc = UseDropout_ThreeLayerNet.accuracy(dataset['train_X'], dataset['one_hot_train'])\n",
    "            test_acc = UseDropout_ThreeLayerNet.accuracy(dataset['test_X'], dataset['one_hot_test'])\n",
    "            print(i, '\\t번째 Loss = ', Loss)\n",
    "            print(i, '\\t번째 Train_Accuracy : ', train_acc)\n",
    "            print(i, '\\t번째 Test_Accuracy : ', test_acc)\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            Loss_list.append(Loss)\n",
    "            \n",
    "    return UseDropout_ThreeLayerNet, train_acc_list, test_acc_list, Loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#과제 채점을 위한 세팅\n",
    "train_X, train_label, test_X, test_label = train_test_split(mnist)\n",
    "\n",
    "one_hot_train = one_hot_encoding(train_label)\n",
    "one_hot_test = one_hot_encoding(test_label)\n",
    "\n",
    "dataset = {}\n",
    "dataset['train_X'] = train_X\n",
    "dataset['test_X'] = test_X\n",
    "dataset['one_hot_train'] = one_hot_train\n",
    "dataset['one_hot_test'] = one_hot_test\n",
    "\n",
    "neournlist = [784, 60, 30, 10]\n",
    "\n",
    "TNN_batchOptimizer = ThreeLayerNet(neournlist)\n",
    "TNN_minibatchOptimizer = copy.deepcopy(TNN_batchOptimizer)\n",
    "TNN_dropOut = UseDropout_ThreeLayerNet(neournlist) # UseDropout_ThreeLayerNet 클래스 객체 생성함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t번째 Loss =  2.3451291573205766\n",
      "0 \t번째 Train_Accuracy :  0.116125\n",
      "0 \t번째 Test_Accuracy :  0.106\n",
      "10 \t번째 Loss =  2.0228148836517645\n",
      "10 \t번째 Train_Accuracy :  0.36725\n",
      "10 \t번째 Test_Accuracy :  0.3415\n",
      "20 \t번째 Loss =  1.665647540592542\n",
      "20 \t번째 Train_Accuracy :  0.523625\n",
      "20 \t번째 Test_Accuracy :  0.5445\n",
      "30 \t번째 Loss =  1.2539896076751587\n",
      "30 \t번째 Train_Accuracy :  0.68475\n",
      "30 \t번째 Test_Accuracy :  0.7165\n",
      "40 \t번째 Loss =  0.9117024380006711\n",
      "40 \t번째 Train_Accuracy :  0.77375\n",
      "40 \t번째 Test_Accuracy :  0.8055\n",
      "50 \t번째 Loss =  0.7191926779627515\n",
      "50 \t번째 Train_Accuracy :  0.815125\n",
      "50 \t번째 Test_Accuracy :  0.846\n",
      "60 \t번째 Loss =  0.6089113520871241\n",
      "60 \t번째 Train_Accuracy :  0.84275\n",
      "60 \t번째 Test_Accuracy :  0.8695\n",
      "70 \t번째 Loss =  0.537988035561975\n",
      "70 \t번째 Train_Accuracy :  0.857\n",
      "70 \t번째 Test_Accuracy :  0.885\n",
      "80 \t번째 Loss =  0.4881950125166446\n",
      "80 \t번째 Train_Accuracy :  0.868\n",
      "80 \t번째 Test_Accuracy :  0.89\n",
      "90 \t번째 Loss =  0.45117660618717764\n",
      "90 \t번째 Train_Accuracy :  0.87675\n",
      "90 \t번째 Test_Accuracy :  0.897\n",
      "100 \t번째 Loss =  0.42358397910980633\n",
      "100 \t번째 Train_Accuracy :  0.881375\n",
      "100 \t번째 Test_Accuracy :  0.9045\n",
      "110 \t번째 Loss =  0.6029693875878004\n",
      "110 \t번째 Train_Accuracy :  0.845875\n",
      "110 \t번째 Test_Accuracy :  0.871\n",
      "120 \t번째 Loss =  0.38705146827767106\n",
      "120 \t번째 Train_Accuracy :  0.891625\n",
      "120 \t번째 Test_Accuracy :  0.911\n",
      "130 \t번째 Loss =  0.36936529540536417\n",
      "130 \t번째 Train_Accuracy :  0.896625\n",
      "130 \t번째 Test_Accuracy :  0.915\n",
      "140 \t번째 Loss =  0.3549675543635893\n",
      "140 \t번째 Train_Accuracy :  0.89975\n",
      "140 \t번째 Test_Accuracy :  0.918\n",
      "150 \t번째 Loss =  0.3427225781630188\n",
      "150 \t번째 Train_Accuracy :  0.902875\n",
      "150 \t번째 Test_Accuracy :  0.919\n",
      "160 \t번째 Loss =  0.3320864135964124\n",
      "160 \t번째 Train_Accuracy :  0.905125\n",
      "160 \t번째 Test_Accuracy :  0.922\n",
      "170 \t번째 Loss =  0.3226996962426143\n",
      "170 \t번째 Train_Accuracy :  0.90775\n",
      "170 \t번째 Test_Accuracy :  0.9225\n",
      "180 \t번째 Loss =  0.31430899940702184\n",
      "180 \t번째 Train_Accuracy :  0.910375\n",
      "180 \t번째 Test_Accuracy :  0.923\n",
      "190 \t번째 Loss =  0.3067371805844348\n",
      "190 \t번째 Train_Accuracy :  0.91225\n",
      "190 \t번째 Test_Accuracy :  0.923\n",
      "200 \t번째 Loss =  0.30004381813450876\n",
      "200 \t번째 Train_Accuracy :  0.91375\n",
      "200 \t번째 Test_Accuracy :  0.924\n",
      "210 \t번째 Loss =  0.30060934200787537\n",
      "210 \t번째 Train_Accuracy :  0.910625\n",
      "210 \t번째 Test_Accuracy :  0.926\n",
      "220 \t번째 Loss =  0.6150621372696276\n",
      "220 \t번째 Train_Accuracy :  0.8655\n",
      "220 \t번째 Test_Accuracy :  0.89\n",
      "230 \t번째 Loss =  0.28547687192048066\n",
      "230 \t번째 Train_Accuracy :  0.918875\n",
      "230 \t번째 Test_Accuracy :  0.9255\n",
      "240 \t번째 Loss =  0.27940580293932205\n",
      "240 \t번째 Train_Accuracy :  0.920875\n",
      "240 \t번째 Test_Accuracy :  0.9275\n",
      "250 \t번째 Loss =  0.27401454784203727\n",
      "250 \t번째 Train_Accuracy :  0.921875\n",
      "250 \t번째 Test_Accuracy :  0.9275\n",
      "260 \t번째 Loss =  0.269066872345343\n",
      "260 \t번째 Train_Accuracy :  0.924\n",
      "260 \t번째 Test_Accuracy :  0.928\n",
      "270 \t번째 Loss =  0.26445579118718826\n",
      "270 \t번째 Train_Accuracy :  0.924625\n",
      "270 \t번째 Test_Accuracy :  0.9285\n",
      "280 \t번째 Loss =  0.26011806854315317\n",
      "280 \t번째 Train_Accuracy :  0.92575\n",
      "280 \t번째 Test_Accuracy :  0.9305\n",
      "290 \t번째 Loss =  0.2560104199504676\n",
      "290 \t번째 Train_Accuracy :  0.927\n",
      "290 \t번째 Test_Accuracy :  0.932\n",
      "300 \t번째 Loss =  0.2521006777530326\n",
      "300 \t번째 Train_Accuracy :  0.928375\n",
      "300 \t번째 Test_Accuracy :  0.932\n",
      "310 \t번째 Loss =  0.24836374823188845\n",
      "310 \t번째 Train_Accuracy :  0.928875\n",
      "310 \t번째 Test_Accuracy :  0.9325\n",
      "320 \t번째 Loss =  0.24477942634652358\n",
      "320 \t번째 Train_Accuracy :  0.930375\n",
      "320 \t번째 Test_Accuracy :  0.933\n",
      "330 \t번째 Loss =  0.24133107256613712\n",
      "330 \t번째 Train_Accuracy :  0.931125\n",
      "330 \t번째 Test_Accuracy :  0.933\n",
      "340 \t번째 Loss =  0.2380047490311079\n",
      "340 \t번째 Train_Accuracy :  0.9325\n",
      "340 \t번째 Test_Accuracy :  0.933\n",
      "350 \t번째 Loss =  0.23478862501073536\n",
      "350 \t번째 Train_Accuracy :  0.933125\n",
      "350 \t번째 Test_Accuracy :  0.933\n",
      "360 \t번째 Loss =  0.2316725510530402\n",
      "360 \t번째 Train_Accuracy :  0.934125\n",
      "360 \t번째 Test_Accuracy :  0.9335\n",
      "370 \t번째 Loss =  0.22864774407391059\n",
      "370 \t번째 Train_Accuracy :  0.935125\n",
      "370 \t번째 Test_Accuracy :  0.934\n",
      "380 \t번째 Loss =  0.22570654827258396\n",
      "380 \t번째 Train_Accuracy :  0.93575\n",
      "380 \t번째 Test_Accuracy :  0.934\n",
      "390 \t번째 Loss =  0.2228422496587183\n",
      "390 \t번째 Train_Accuracy :  0.936875\n",
      "390 \t번째 Test_Accuracy :  0.9335\n",
      "400 \t번째 Loss =  0.2200489301309425\n",
      "400 \t번째 Train_Accuracy :  0.937375\n",
      "400 \t번째 Test_Accuracy :  0.9345\n",
      "410 \t번째 Loss =  0.21732135507657518\n",
      "410 \t번째 Train_Accuracy :  0.938\n",
      "410 \t번째 Test_Accuracy :  0.934\n",
      "420 \t번째 Loss =  0.21465491393689548\n",
      "420 \t번째 Train_Accuracy :  0.93875\n",
      "420 \t번째 Test_Accuracy :  0.9345\n",
      "430 \t번째 Loss =  0.21204579806083695\n",
      "430 \t번째 Train_Accuracy :  0.939625\n",
      "430 \t번째 Test_Accuracy :  0.933\n",
      "440 \t번째 Loss =  0.20949285350859714\n",
      "440 \t번째 Train_Accuracy :  0.940625\n",
      "440 \t번째 Test_Accuracy :  0.934\n",
      "450 \t번째 Loss =  0.20701268334058828\n",
      "450 \t번째 Train_Accuracy :  0.942\n",
      "450 \t번째 Test_Accuracy :  0.9345\n",
      "460 \t번째 Loss =  0.2047658772490206\n",
      "460 \t번째 Train_Accuracy :  0.943\n",
      "460 \t번째 Test_Accuracy :  0.9345\n",
      "470 \t번째 Loss =  0.2041869727643167\n",
      "470 \t번째 Train_Accuracy :  0.941875\n",
      "470 \t번째 Test_Accuracy :  0.9355\n",
      "480 \t번째 Loss =  0.22125966034687045\n",
      "480 \t번째 Train_Accuracy :  0.931875\n",
      "480 \t번째 Test_Accuracy :  0.936\n",
      "490 \t번째 Loss =  0.49772543073545983\n",
      "490 \t번째 Train_Accuracy :  0.87825\n",
      "490 \t번째 Test_Accuracy :  0.8965\n",
      "500 \t번째 Loss =  0.19754966043012429\n",
      "500 \t번째 Train_Accuracy :  0.943375\n",
      "500 \t번째 Test_Accuracy :  0.937\n",
      "510 \t번째 Loss =  0.19442200482760102\n",
      "510 \t번째 Train_Accuracy :  0.944625\n",
      "510 \t번째 Test_Accuracy :  0.9365\n",
      "520 \t번째 Loss =  0.1917786109103286\n",
      "520 \t번째 Train_Accuracy :  0.9455\n",
      "520 \t번째 Test_Accuracy :  0.937\n",
      "530 \t번째 Loss =  0.18933834856397358\n",
      "530 \t번째 Train_Accuracy :  0.946875\n",
      "530 \t번째 Test_Accuracy :  0.9365\n",
      "540 \t번째 Loss =  0.1870072315067855\n",
      "540 \t번째 Train_Accuracy :  0.9475\n",
      "540 \t번째 Test_Accuracy :  0.937\n",
      "550 \t번째 Loss =  0.1847482005366195\n",
      "550 \t번째 Train_Accuracy :  0.948\n",
      "550 \t번째 Test_Accuracy :  0.937\n",
      "560 \t번째 Loss =  0.18254412970835718\n",
      "560 \t번째 Train_Accuracy :  0.94875\n",
      "560 \t번째 Test_Accuracy :  0.937\n",
      "570 \t번째 Loss =  0.18038576497959746\n",
      "570 \t번째 Train_Accuracy :  0.9495\n",
      "570 \t번째 Test_Accuracy :  0.937\n",
      "580 \t번째 Loss =  0.178267369373248\n",
      "580 \t번째 Train_Accuracy :  0.9495\n",
      "580 \t번째 Test_Accuracy :  0.937\n",
      "590 \t번째 Loss =  0.1761849894503128\n",
      "590 \t번째 Train_Accuracy :  0.95\n",
      "590 \t번째 Test_Accuracy :  0.9375\n",
      "600 \t번째 Loss =  0.17413569063504247\n",
      "600 \t번째 Train_Accuracy :  0.950625\n",
      "600 \t번째 Test_Accuracy :  0.938\n",
      "610 \t번째 Loss =  0.17211718168050058\n",
      "610 \t번째 Train_Accuracy :  0.951625\n",
      "610 \t번째 Test_Accuracy :  0.9375\n",
      "620 \t번째 Loss =  0.1701276101107435\n",
      "620 \t번째 Train_Accuracy :  0.952375\n",
      "620 \t번째 Test_Accuracy :  0.937\n",
      "630 \t번째 Loss =  0.16816544024531965\n",
      "630 \t번째 Train_Accuracy :  0.953\n",
      "630 \t번째 Test_Accuracy :  0.9365\n",
      "640 \t번째 Loss =  0.16622937495796045\n",
      "640 \t번째 Train_Accuracy :  0.954\n",
      "640 \t번째 Test_Accuracy :  0.9365\n",
      "650 \t번째 Loss =  0.164318302561247\n",
      "650 \t번째 Train_Accuracy :  0.95475\n",
      "650 \t번째 Test_Accuracy :  0.9365\n",
      "660 \t번째 Loss =  0.16243125912304157\n",
      "660 \t번째 Train_Accuracy :  0.95525\n",
      "660 \t번째 Test_Accuracy :  0.9365\n",
      "670 \t번째 Loss =  0.16056740077201956\n",
      "670 \t번째 Train_Accuracy :  0.956\n",
      "670 \t번째 Test_Accuracy :  0.9365\n",
      "680 \t번째 Loss =  0.1587259827416574\n",
      "680 \t번째 Train_Accuracy :  0.956625\n",
      "680 \t번째 Test_Accuracy :  0.937\n",
      "690 \t번째 Loss =  0.15690634311392915\n",
      "690 \t번째 Train_Accuracy :  0.957375\n",
      "690 \t번째 Test_Accuracy :  0.9375\n",
      "700 \t번째 Loss =  0.15510788993411717\n",
      "700 \t번째 Train_Accuracy :  0.957875\n",
      "700 \t번째 Test_Accuracy :  0.938\n",
      "710 \t번째 Loss =  0.15333009080397034\n",
      "710 \t번째 Train_Accuracy :  0.958625\n",
      "710 \t번째 Test_Accuracy :  0.9385\n",
      "720 \t번째 Loss =  0.15157246433775753\n",
      "720 \t번째 Train_Accuracy :  0.95925\n",
      "720 \t번째 Test_Accuracy :  0.9385\n",
      "730 \t번째 Loss =  0.14983457304737027\n",
      "730 \t번째 Train_Accuracy :  0.95975\n",
      "730 \t번째 Test_Accuracy :  0.9385\n",
      "740 \t번째 Loss =  0.14811601734447327\n",
      "740 \t번째 Train_Accuracy :  0.960125\n",
      "740 \t번째 Test_Accuracy :  0.9395\n",
      "750 \t번째 Loss =  0.14641643043125696\n",
      "750 \t번째 Train_Accuracy :  0.96025\n",
      "750 \t번째 Test_Accuracy :  0.94\n",
      "760 \t번째 Loss =  0.14473547390989827\n",
      "760 \t번째 Train_Accuracy :  0.960625\n",
      "760 \t번째 Test_Accuracy :  0.9405\n",
      "770 \t번째 Loss =  0.14307283398277096\n",
      "770 \t번째 Train_Accuracy :  0.960875\n",
      "770 \t번째 Test_Accuracy :  0.9405\n",
      "780 \t번째 Loss =  0.14142821814616047\n",
      "780 \t번째 Train_Accuracy :  0.96175\n",
      "780 \t번째 Test_Accuracy :  0.9405\n",
      "790 \t번째 Loss =  0.1398013523032095\n",
      "790 \t번째 Train_Accuracy :  0.961875\n",
      "790 \t번째 Test_Accuracy :  0.941\n",
      "800 \t번째 Loss =  0.13819197823928606\n",
      "800 \t번째 Train_Accuracy :  0.9625\n",
      "800 \t번째 Test_Accuracy :  0.941\n",
      "810 \t번째 Loss =  0.13659985141636127\n",
      "810 \t번째 Train_Accuracy :  0.962625\n",
      "810 \t번째 Test_Accuracy :  0.941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820 \t번째 Loss =  0.13502473905322768\n",
      "820 \t번째 Train_Accuracy :  0.96325\n",
      "820 \t번째 Test_Accuracy :  0.941\n",
      "830 \t번째 Loss =  0.13346641846607227\n",
      "830 \t번째 Train_Accuracy :  0.963625\n",
      "830 \t번째 Test_Accuracy :  0.9415\n",
      "840 \t번째 Loss =  0.1319246756494694\n",
      "840 \t번째 Train_Accuracy :  0.964\n",
      "840 \t번째 Test_Accuracy :  0.9415\n",
      "850 \t번째 Loss =  0.13039930408160155\n",
      "850 \t번째 Train_Accuracy :  0.96425\n",
      "850 \t번째 Test_Accuracy :  0.942\n",
      "860 \t번째 Loss =  0.12889010373976145\n",
      "860 \t번째 Train_Accuracy :  0.9655\n",
      "860 \t번째 Test_Accuracy :  0.943\n",
      "870 \t번째 Loss =  0.12739688031322086\n",
      "870 \t번째 Train_Accuracy :  0.9655\n",
      "870 \t번째 Test_Accuracy :  0.943\n",
      "880 \t번째 Loss =  0.12591944460067214\n",
      "880 \t번째 Train_Accuracy :  0.96575\n",
      "880 \t번째 Test_Accuracy :  0.943\n",
      "890 \t번째 Loss =  0.12445761207891672\n",
      "890 \t번째 Train_Accuracy :  0.966\n",
      "890 \t번째 Test_Accuracy :  0.943\n",
      "900 \t번째 Loss =  0.12301120262853713\n",
      "900 \t번째 Train_Accuracy :  0.9665\n",
      "900 \t번째 Test_Accuracy :  0.943\n",
      "910 \t번째 Loss =  0.12158004040113814\n",
      "910 \t번째 Train_Accuracy :  0.966875\n",
      "910 \t번째 Test_Accuracy :  0.943\n",
      "920 \t번째 Loss =  0.12016395381151444\n",
      "920 \t번째 Train_Accuracy :  0.96725\n",
      "920 \t번째 Test_Accuracy :  0.943\n",
      "930 \t번째 Loss =  0.11876277563690174\n",
      "930 \t번째 Train_Accuracy :  0.968625\n",
      "930 \t번째 Test_Accuracy :  0.9425\n",
      "940 \t번째 Loss =  0.1173763432043368\n",
      "940 \t번째 Train_Accuracy :  0.969125\n",
      "940 \t번째 Test_Accuracy :  0.943\n",
      "950 \t번째 Loss =  0.11600449864612969\n",
      "950 \t번째 Train_Accuracy :  0.969375\n",
      "950 \t번째 Test_Accuracy :  0.9435\n",
      "960 \t번째 Loss =  0.11464708920252528\n",
      "960 \t번째 Train_Accuracy :  0.97\n",
      "960 \t번째 Test_Accuracy :  0.9435\n",
      "970 \t번째 Loss =  0.11330396754981167\n",
      "970 \t번째 Train_Accuracy :  0.9705\n",
      "970 \t번째 Test_Accuracy :  0.944\n",
      "980 \t번째 Loss =  0.1119749921314023\n",
      "980 \t번째 Train_Accuracy :  0.97075\n",
      "980 \t번째 Test_Accuracy :  0.9435\n",
      "990 \t번째 Loss =  0.11066002746879441\n",
      "990 \t번째 Train_Accuracy :  0.97125\n",
      "990 \t번째 Test_Accuracy :  0.9435\n",
      "1000 \t번째 Loss =  0.10935894442883307\n",
      "1000 \t번째 Train_Accuracy :  0.972125\n",
      "1000 \t번째 Test_Accuracy :  0.9435\n",
      "0 \t번째 Loss =  0.5738017779423208\n",
      "0 \t번째 Train_Accuracy :  0.85175\n",
      "0 \t번째 Test_Accuracy :  0.8685\n",
      "10 \t번째 Loss =  0.09240857433556084\n",
      "10 \t번째 Train_Accuracy :  0.962\n",
      "10 \t번째 Test_Accuracy :  0.9455\n",
      "20 \t번째 Loss =  0.04175587312515071\n",
      "20 \t번째 Train_Accuracy :  0.983375\n",
      "20 \t번째 Test_Accuracy :  0.947\n",
      "30 \t번째 Loss =  0.011754662139523153\n",
      "30 \t번째 Train_Accuracy :  0.99725\n",
      "30 \t번째 Test_Accuracy :  0.955\n",
      "40 \t번째 Loss =  0.02060100337250139\n",
      "40 \t번째 Train_Accuracy :  0.999875\n",
      "40 \t번째 Test_Accuracy :  0.955\n",
      "50 \t번째 Loss =  0.00589678860174121\n",
      "50 \t번째 Train_Accuracy :  1.0\n",
      "50 \t번째 Test_Accuracy :  0.9575\n",
      "60 \t번째 Loss =  0.003046234248537981\n",
      "60 \t번째 Train_Accuracy :  1.0\n",
      "60 \t번째 Test_Accuracy :  0.956\n",
      "70 \t번째 Loss =  0.0015916925334073686\n",
      "70 \t번째 Train_Accuracy :  1.0\n",
      "70 \t번째 Test_Accuracy :  0.9545\n",
      "80 \t번째 Loss =  0.0020304985307556117\n",
      "80 \t번째 Train_Accuracy :  1.0\n",
      "80 \t번째 Test_Accuracy :  0.955\n",
      "90 \t번째 Loss =  0.0031463294176050727\n",
      "90 \t번째 Train_Accuracy :  1.0\n",
      "90 \t번째 Test_Accuracy :  0.957\n",
      "100 \t번째 Loss =  0.0009121333389574415\n",
      "100 \t번째 Train_Accuracy :  1.0\n",
      "100 \t번째 Test_Accuracy :  0.956\n",
      "0 \t번째 Loss =  2.3283689802100973\n",
      "0 \t번째 Train_Accuracy :  0.104625\n",
      "0 \t번째 Test_Accuracy :  0.0915\n",
      "10 \t번째 Loss =  2.1944847716280584\n",
      "10 \t번째 Train_Accuracy :  0.266625\n",
      "10 \t번째 Test_Accuracy :  0.246\n",
      "20 \t번째 Loss =  2.0440392594490002\n",
      "20 \t번째 Train_Accuracy :  0.3905\n",
      "20 \t번째 Test_Accuracy :  0.3695\n",
      "30 \t번째 Loss =  1.8711031646447536\n",
      "30 \t번째 Train_Accuracy :  0.466875\n",
      "30 \t번째 Test_Accuracy :  0.47\n",
      "40 \t번째 Loss =  1.6992629010921119\n",
      "40 \t번째 Train_Accuracy :  0.55675\n",
      "40 \t번째 Test_Accuracy :  0.582\n",
      "50 \t번째 Loss =  1.5238784121427285\n",
      "50 \t번째 Train_Accuracy :  0.64725\n",
      "50 \t번째 Test_Accuracy :  0.687\n",
      "60 \t번째 Loss =  1.3680196866136003\n",
      "60 \t번째 Train_Accuracy :  0.708875\n",
      "60 \t번째 Test_Accuracy :  0.7385\n",
      "70 \t번째 Loss =  1.2317601067755914\n",
      "70 \t번째 Train_Accuracy :  0.749625\n",
      "70 \t번째 Test_Accuracy :  0.7855\n",
      "80 \t번째 Loss =  1.114650977587161\n",
      "80 \t번째 Train_Accuracy :  0.77925\n",
      "80 \t번째 Test_Accuracy :  0.819\n",
      "90 \t번째 Loss =  1.021646589237635\n",
      "90 \t번째 Train_Accuracy :  0.795625\n",
      "90 \t번째 Test_Accuracy :  0.836\n",
      "100 \t번째 Loss =  0.9544105183768747\n",
      "100 \t번째 Train_Accuracy :  0.815875\n",
      "100 \t번째 Test_Accuracy :  0.848\n",
      "110 \t번째 Loss =  0.8991163648441772\n",
      "110 \t번째 Train_Accuracy :  0.83\n",
      "110 \t번째 Test_Accuracy :  0.8615\n",
      "120 \t번째 Loss =  0.862911906952603\n",
      "120 \t번째 Train_Accuracy :  0.838375\n",
      "120 \t번째 Test_Accuracy :  0.873\n",
      "130 \t번째 Loss =  0.8164748212874724\n",
      "130 \t번째 Train_Accuracy :  0.846375\n",
      "130 \t번째 Test_Accuracy :  0.88\n",
      "140 \t번째 Loss =  0.7835874172599321\n",
      "140 \t번째 Train_Accuracy :  0.853125\n",
      "140 \t번째 Test_Accuracy :  0.8865\n",
      "150 \t번째 Loss =  0.7566494699346518\n",
      "150 \t번째 Train_Accuracy :  0.859625\n",
      "150 \t번째 Test_Accuracy :  0.8905\n",
      "160 \t번째 Loss =  0.7368525652197887\n",
      "160 \t번째 Train_Accuracy :  0.864125\n",
      "160 \t번째 Test_Accuracy :  0.8905\n",
      "170 \t번째 Loss =  0.7156710106082987\n",
      "170 \t번째 Train_Accuracy :  0.86825\n",
      "170 \t번째 Test_Accuracy :  0.896\n",
      "180 \t번째 Loss =  0.6859470950351403\n",
      "180 \t번째 Train_Accuracy :  0.873\n",
      "180 \t번째 Test_Accuracy :  0.8985\n",
      "190 \t번째 Loss =  0.6802967930236751\n",
      "190 \t번째 Train_Accuracy :  0.875375\n",
      "190 \t번째 Test_Accuracy :  0.902\n",
      "200 \t번째 Loss =  0.6480305906422872\n",
      "200 \t번째 Train_Accuracy :  0.87775\n",
      "200 \t번째 Test_Accuracy :  0.903\n",
      "210 \t번째 Loss =  0.651029586227693\n",
      "210 \t번째 Train_Accuracy :  0.88025\n",
      "210 \t번째 Test_Accuracy :  0.9065\n",
      "220 \t번째 Loss =  0.6290454360138702\n",
      "220 \t번째 Train_Accuracy :  0.8825\n",
      "220 \t번째 Test_Accuracy :  0.9085\n",
      "230 \t번째 Loss =  0.6136049575957886\n",
      "230 \t번째 Train_Accuracy :  0.88625\n",
      "230 \t번째 Test_Accuracy :  0.909\n",
      "240 \t번째 Loss =  0.598392849596908\n",
      "240 \t번째 Train_Accuracy :  0.889\n",
      "240 \t번째 Test_Accuracy :  0.911\n",
      "250 \t번째 Loss =  0.5886689400963657\n",
      "250 \t번째 Train_Accuracy :  0.889625\n",
      "250 \t번째 Test_Accuracy :  0.9125\n",
      "260 \t번째 Loss =  0.5772106868107257\n",
      "260 \t번째 Train_Accuracy :  0.89175\n",
      "260 \t번째 Test_Accuracy :  0.9135\n",
      "270 \t번째 Loss =  0.5589465173344628\n",
      "270 \t번째 Train_Accuracy :  0.893125\n",
      "270 \t번째 Test_Accuracy :  0.915\n",
      "280 \t번째 Loss =  0.5459685489558646\n",
      "280 \t번째 Train_Accuracy :  0.894875\n",
      "280 \t번째 Test_Accuracy :  0.9175\n",
      "290 \t번째 Loss =  0.5525662502601179\n",
      "290 \t번째 Train_Accuracy :  0.89625\n",
      "290 \t번째 Test_Accuracy :  0.918\n",
      "300 \t번째 Loss =  0.531264583534452\n",
      "300 \t번째 Train_Accuracy :  0.8975\n",
      "300 \t번째 Test_Accuracy :  0.9195\n",
      "310 \t번째 Loss =  0.5198676324540459\n",
      "310 \t번째 Train_Accuracy :  0.89825\n",
      "310 \t번째 Test_Accuracy :  0.9205\n",
      "320 \t번째 Loss =  0.5147176848886694\n",
      "320 \t번째 Train_Accuracy :  0.9005\n",
      "320 \t번째 Test_Accuracy :  0.921\n",
      "330 \t번째 Loss =  0.5226247415880938\n",
      "330 \t번째 Train_Accuracy :  0.901875\n",
      "330 \t번째 Test_Accuracy :  0.922\n",
      "340 \t번째 Loss =  0.49966644569099844\n",
      "340 \t번째 Train_Accuracy :  0.902875\n",
      "340 \t번째 Test_Accuracy :  0.9235\n",
      "350 \t번째 Loss =  0.48887507954565146\n",
      "350 \t번째 Train_Accuracy :  0.9045\n",
      "350 \t번째 Test_Accuracy :  0.925\n",
      "360 \t번째 Loss =  0.4871609450964479\n",
      "360 \t번째 Train_Accuracy :  0.905625\n",
      "360 \t번째 Test_Accuracy :  0.923\n",
      "370 \t번째 Loss =  0.4936659422029358\n",
      "370 \t번째 Train_Accuracy :  0.9065\n",
      "370 \t번째 Test_Accuracy :  0.9225\n",
      "380 \t번째 Loss =  0.4831158635536818\n",
      "380 \t번째 Train_Accuracy :  0.907875\n",
      "380 \t번째 Test_Accuracy :  0.925\n",
      "390 \t번째 Loss =  0.4719700066786998\n",
      "390 \t번째 Train_Accuracy :  0.909125\n",
      "390 \t번째 Test_Accuracy :  0.9255\n",
      "400 \t번째 Loss =  0.47075594392990966\n",
      "400 \t번째 Train_Accuracy :  0.9095\n",
      "400 \t번째 Test_Accuracy :  0.926\n",
      "410 \t번째 Loss =  0.46585697380497637\n",
      "410 \t번째 Train_Accuracy :  0.910125\n",
      "410 \t번째 Test_Accuracy :  0.926\n",
      "420 \t번째 Loss =  0.449548086595531\n",
      "420 \t번째 Train_Accuracy :  0.9115\n",
      "420 \t번째 Test_Accuracy :  0.9275\n",
      "430 \t번째 Loss =  0.4566570713119023\n",
      "430 \t번째 Train_Accuracy :  0.912875\n",
      "430 \t번째 Test_Accuracy :  0.9285\n",
      "440 \t번째 Loss =  0.4478818699330655\n",
      "440 \t번째 Train_Accuracy :  0.913875\n",
      "440 \t번째 Test_Accuracy :  0.9295\n",
      "450 \t번째 Loss =  0.44314587820139\n",
      "450 \t번째 Train_Accuracy :  0.915125\n",
      "450 \t번째 Test_Accuracy :  0.93\n",
      "460 \t번째 Loss =  0.4395018217730472\n",
      "460 \t번째 Train_Accuracy :  0.915875\n",
      "460 \t번째 Test_Accuracy :  0.93\n",
      "470 \t번째 Loss =  0.4303299477422502\n",
      "470 \t번째 Train_Accuracy :  0.915875\n",
      "470 \t번째 Test_Accuracy :  0.9315\n",
      "480 \t번째 Loss =  0.4254414455386504\n",
      "480 \t번째 Train_Accuracy :  0.91625\n",
      "480 \t번째 Test_Accuracy :  0.9315\n",
      "490 \t번째 Loss =  0.43652339016975505\n",
      "490 \t번째 Train_Accuracy :  0.917\n",
      "490 \t번째 Test_Accuracy :  0.932\n",
      "500 \t번째 Loss =  0.41976853421418786\n",
      "500 \t번째 Train_Accuracy :  0.917125\n",
      "500 \t번째 Test_Accuracy :  0.9325\n",
      "510 \t번째 Loss =  0.4234152611343523\n",
      "510 \t번째 Train_Accuracy :  0.91825\n",
      "510 \t번째 Test_Accuracy :  0.932\n",
      "520 \t번째 Loss =  0.40843722612872135\n",
      "520 \t번째 Train_Accuracy :  0.91925\n",
      "520 \t번째 Test_Accuracy :  0.934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "530 \t번째 Loss =  0.410728773088427\n",
      "530 \t번째 Train_Accuracy :  0.92025\n",
      "530 \t번째 Test_Accuracy :  0.933\n",
      "540 \t번째 Loss =  0.40616808965767753\n",
      "540 \t번째 Train_Accuracy :  0.920625\n",
      "540 \t번째 Test_Accuracy :  0.933\n",
      "550 \t번째 Loss =  0.40536940776262936\n",
      "550 \t번째 Train_Accuracy :  0.921375\n",
      "550 \t번째 Test_Accuracy :  0.9335\n",
      "560 \t번째 Loss =  0.4054704737441872\n",
      "560 \t번째 Train_Accuracy :  0.921375\n",
      "560 \t번째 Test_Accuracy :  0.933\n",
      "570 \t번째 Loss =  0.389052098265405\n",
      "570 \t번째 Train_Accuracy :  0.92275\n",
      "570 \t번째 Test_Accuracy :  0.933\n",
      "580 \t번째 Loss =  0.3847549616112276\n",
      "580 \t번째 Train_Accuracy :  0.923125\n",
      "580 \t번째 Test_Accuracy :  0.933\n",
      "590 \t번째 Loss =  0.395003025950307\n",
      "590 \t번째 Train_Accuracy :  0.923875\n",
      "590 \t번째 Test_Accuracy :  0.933\n",
      "600 \t번째 Loss =  0.3882177194272729\n",
      "600 \t번째 Train_Accuracy :  0.924375\n",
      "600 \t번째 Test_Accuracy :  0.9335\n",
      "610 \t번째 Loss =  0.38161537997866846\n",
      "610 \t번째 Train_Accuracy :  0.92525\n",
      "610 \t번째 Test_Accuracy :  0.9335\n",
      "620 \t번째 Loss =  0.3769423288049374\n",
      "620 \t번째 Train_Accuracy :  0.926\n",
      "620 \t번째 Test_Accuracy :  0.934\n",
      "630 \t번째 Loss =  0.3826574661844203\n",
      "630 \t번째 Train_Accuracy :  0.926375\n",
      "630 \t번째 Test_Accuracy :  0.9345\n",
      "640 \t번째 Loss =  0.37451688607865075\n",
      "640 \t번째 Train_Accuracy :  0.927125\n",
      "640 \t번째 Test_Accuracy :  0.9345\n",
      "650 \t번째 Loss =  0.37980769089672783\n",
      "650 \t번째 Train_Accuracy :  0.927375\n",
      "650 \t번째 Test_Accuracy :  0.9345\n",
      "660 \t번째 Loss =  0.36789920899861833\n",
      "660 \t번째 Train_Accuracy :  0.928125\n",
      "660 \t번째 Test_Accuracy :  0.9345\n",
      "670 \t번째 Loss =  0.3664667710049283\n",
      "670 \t번째 Train_Accuracy :  0.929375\n",
      "670 \t번째 Test_Accuracy :  0.9345\n",
      "680 \t번째 Loss =  0.3552581594915888\n",
      "680 \t번째 Train_Accuracy :  0.93\n",
      "680 \t번째 Test_Accuracy :  0.9365\n",
      "690 \t번째 Loss =  0.3608167012488151\n",
      "690 \t번째 Train_Accuracy :  0.93075\n",
      "690 \t번째 Test_Accuracy :  0.9365\n",
      "700 \t번째 Loss =  0.36590534738290437\n",
      "700 \t번째 Train_Accuracy :  0.930875\n",
      "700 \t번째 Test_Accuracy :  0.937\n",
      "710 \t번째 Loss =  0.3608160720056847\n",
      "710 \t번째 Train_Accuracy :  0.93125\n",
      "710 \t번째 Test_Accuracy :  0.937\n",
      "720 \t번째 Loss =  0.35100520790487466\n",
      "720 \t번째 Train_Accuracy :  0.931625\n",
      "720 \t번째 Test_Accuracy :  0.937\n",
      "730 \t번째 Loss =  0.3583191853624358\n",
      "730 \t번째 Train_Accuracy :  0.932375\n",
      "730 \t번째 Test_Accuracy :  0.937\n",
      "740 \t번째 Loss =  0.3485482767026081\n",
      "740 \t번째 Train_Accuracy :  0.93275\n",
      "740 \t번째 Test_Accuracy :  0.938\n",
      "750 \t번째 Loss =  0.35284532997912604\n",
      "750 \t번째 Train_Accuracy :  0.933125\n",
      "750 \t번째 Test_Accuracy :  0.939\n",
      "760 \t번째 Loss =  0.34390550527111385\n",
      "760 \t번째 Train_Accuracy :  0.934125\n",
      "760 \t번째 Test_Accuracy :  0.9395\n",
      "770 \t번째 Loss =  0.3377183549826807\n",
      "770 \t번째 Train_Accuracy :  0.9345\n",
      "770 \t번째 Test_Accuracy :  0.9395\n",
      "780 \t번째 Loss =  0.3376788761195192\n",
      "780 \t번째 Train_Accuracy :  0.93475\n",
      "780 \t번째 Test_Accuracy :  0.9395\n",
      "790 \t번째 Loss =  0.3333475188686137\n",
      "790 \t번째 Train_Accuracy :  0.935625\n",
      "790 \t번째 Test_Accuracy :  0.939\n",
      "800 \t번째 Loss =  0.3308722126390068\n",
      "800 \t번째 Train_Accuracy :  0.935875\n",
      "800 \t번째 Test_Accuracy :  0.9395\n",
      "810 \t번째 Loss =  0.3331691399066267\n",
      "810 \t번째 Train_Accuracy :  0.935875\n",
      "810 \t번째 Test_Accuracy :  0.939\n",
      "820 \t번째 Loss =  0.3261504302509915\n",
      "820 \t번째 Train_Accuracy :  0.937\n",
      "820 \t번째 Test_Accuracy :  0.9395\n",
      "830 \t번째 Loss =  0.3324004836091788\n",
      "830 \t번째 Train_Accuracy :  0.93725\n",
      "830 \t번째 Test_Accuracy :  0.9395\n",
      "840 \t번째 Loss =  0.3217125314617709\n",
      "840 \t번째 Train_Accuracy :  0.937625\n",
      "840 \t번째 Test_Accuracy :  0.9395\n",
      "850 \t번째 Loss =  0.3235832972393798\n",
      "850 \t번째 Train_Accuracy :  0.938375\n",
      "850 \t번째 Test_Accuracy :  0.939\n",
      "860 \t번째 Loss =  0.32664385956223124\n",
      "860 \t번째 Train_Accuracy :  0.938875\n",
      "860 \t번째 Test_Accuracy :  0.9395\n",
      "870 \t번째 Loss =  0.31864153723568767\n",
      "870 \t번째 Train_Accuracy :  0.939\n",
      "870 \t번째 Test_Accuracy :  0.9395\n",
      "880 \t번째 Loss =  0.31398180838669104\n",
      "880 \t번째 Train_Accuracy :  0.9395\n",
      "880 \t번째 Test_Accuracy :  0.9405\n",
      "890 \t번째 Loss =  0.31397852032316564\n",
      "890 \t번째 Train_Accuracy :  0.939875\n",
      "890 \t번째 Test_Accuracy :  0.941\n",
      "900 \t번째 Loss =  0.315045668810272\n",
      "900 \t번째 Train_Accuracy :  0.94025\n",
      "900 \t번째 Test_Accuracy :  0.94\n",
      "910 \t번째 Loss =  0.3119117058623029\n",
      "910 \t번째 Train_Accuracy :  0.9405\n",
      "910 \t번째 Test_Accuracy :  0.94\n",
      "920 \t번째 Loss =  0.3090508910411939\n",
      "920 \t번째 Train_Accuracy :  0.940875\n",
      "920 \t번째 Test_Accuracy :  0.9405\n",
      "930 \t번째 Loss =  0.3147792463821933\n",
      "930 \t번째 Train_Accuracy :  0.941375\n",
      "930 \t번째 Test_Accuracy :  0.941\n",
      "940 \t번째 Loss =  0.3084822460798671\n",
      "940 \t번째 Train_Accuracy :  0.942375\n",
      "940 \t번째 Test_Accuracy :  0.941\n",
      "950 \t번째 Loss =  0.29803502327496356\n",
      "950 \t번째 Train_Accuracy :  0.942875\n",
      "950 \t번째 Test_Accuracy :  0.9405\n",
      "960 \t번째 Loss =  0.3053586270792683\n",
      "960 \t번째 Train_Accuracy :  0.943375\n",
      "960 \t번째 Test_Accuracy :  0.94\n",
      "970 \t번째 Loss =  0.30282695829741874\n",
      "970 \t번째 Train_Accuracy :  0.9435\n",
      "970 \t번째 Test_Accuracy :  0.94\n",
      "980 \t번째 Loss =  0.29743165088709905\n",
      "980 \t번째 Train_Accuracy :  0.943875\n",
      "980 \t번째 Test_Accuracy :  0.9405\n",
      "990 \t번째 Loss =  0.302145924057366\n",
      "990 \t번째 Train_Accuracy :  0.944125\n",
      "990 \t번째 Test_Accuracy :  0.9405\n",
      "1000 \t번째 Loss =  0.2905225308135513\n",
      "1000 \t번째 Train_Accuracy :  0.94425\n",
      "1000 \t번째 Test_Accuracy :  0.9405\n"
     ]
    }
   ],
   "source": [
    "#채점은 이 것의 결과값으로 할 예정입니다. \n",
    "\n",
    "trained_batch, tb_train_acc_list, tb_test_acc_list, tb_loss_list = batchOptimization(dataset, TNN_batchOptimizer, 0.1, 1000)\n",
    "trained_minibatch, tmb_train_acc_list, tmb_test_acc_list, tb_loss_list = minibatch_Optimization(dataset, TNN_minibatchOptimizer, 0.1, epoch=100, batch_size=100)\n",
    "trained_dropout, td_train_acc_list, td_test_acc_list, td_loss_list = dropout_use_Optimizer(dataset, TNN_dropOut, 0.1, 1000, 0.25, 0.15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
