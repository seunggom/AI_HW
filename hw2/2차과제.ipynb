{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = np.loadtxt('mnist.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(csv_dataset):\n",
    "    '''\n",
    "    csv_dataset의 shape는 (10000, 785)이다.\n",
    "    총 만 개의 데이터가 있고, 각 데이터는 레이블값(1개), 픽셀값(784개)들로 이루어져 있다.\n",
    "    train_set 개수 : test_set 개수 = 80 : 20 의 비율로 데이터를 분할하고 레이블값과 픽셀값으로 한 번 더 분할하면,\n",
    "    train_X.shape = (8000, 784)\n",
    "    train_T.shape = (8000, 1)\n",
    "    test_X.shape = (2000, 784)\n",
    "    test_T.shape = (2000, 1) 이다.\n",
    "    '''\n",
    "    train_X = csv_dataset[:8000, 1:]\n",
    "    train_T = csv_dataset[:8000, 0]\n",
    "    test_X = csv_dataset[8000:, 1:]\n",
    "    test_T = csv_dataset[8000:, 0]  \n",
    "    \n",
    "    return train_X, train_T, test_X, test_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(T): # T is data의 label\n",
    "    one_hot_label = np.zeros([T.shape[0],10])\n",
    "    T = T.astype(np.uint8)\n",
    "    one_hot_label[np.arange(T.shape[0]), T] = 1\n",
    "    \n",
    "    '''\n",
    "    먼저 [데이터개수, 10] 크기의 배열을 만든다\n",
    "    각 데이터마다의 레이블값과 같은 인덱스열에 1의 값을 넣어주어야 하는데,\n",
    "    T에 들어있는 값은 float형이기 때문에, 형변환을 하지 않고 3번째 줄을 실행하면\n",
    "    IndexError: arrays used as indices must be of integer (or boolean) type 와 같은 에러가 발생한다.\n",
    "    그래서 np.astype을 이용해 int형으로 변환해주었다.\n",
    "    \n",
    "    이 함수의 예를 들자면, T=[7,2]일 때 one_hot_label은\n",
    "    [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
    "     [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 가 나오게 된다.\n",
    "    '''\n",
    "    \n",
    "    return one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(ScoreMatrix): # 제공.\n",
    "\n",
    "    if ScoreMatrix.ndim == 2:\n",
    "        temp = ScoreMatrix\n",
    "        temp = temp - np.max(temp, axis=1, keepdims=True)\n",
    "        y_predict = np.exp(temp) / np.sum(np.exp(temp), axis=1, keepdims=True)\n",
    "        return y_predict\n",
    "    temp = ScoreMatrix - np.max(ScoreMatrix, axis=0)\n",
    "    expX = np.exp(temp)\n",
    "    y_predict = expX / np.sum(expX)\n",
    "    return y_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setParam_He(neuronlist):\n",
    "    \n",
    "    np.random.seed(1) # seed값 고정을 통해 input이 같으면 언제나 같은 Weight와 bias를 출력하기 위한 함수\n",
    "    \n",
    "    '''\n",
    "    input layer neuron, hidden layer1 neuron, hidden layer2 neuron과 연산을 할 각각의 Weight가 필요하다.\n",
    "    이 Weight값과 Bias 값을 He의 방법으로 초기화를 한다.\n",
    "    He의 방법을 이용한 Weight의 초기값이란 앞 계층의 노드가 n 개일 때, 표준편차가 (2/n)^0.5 인 정규분포를 사용하는 것을 말한다.\n",
    "    input layer neuron의 크기는 [데이터 개수, 784],\n",
    "    hidden layer1 neuron의 크기는 [데이터 개수, 60],\n",
    "    hidden layer2 neuron의 크기는 [데이터 개수, 30],\n",
    "    output layer neuron의 크기는 [데이터 개수, 10] 이므로\n",
    "    필요한 Weight의 크기는 순서대로 [784, 60], [60, 30], [30, 10]이 될 것이다.\n",
    "    '''\n",
    "    \n",
    "    W1 = np.random.randn(neuronlist[0], neuronlist[1]) / np.sqrt(neuronlist[0]/2)\n",
    "    W2 = np.random.randn(neuronlist[1], neuronlist[2]) / np.sqrt(neuronlist[1]/2)\n",
    "    W3 = np.random.randn(neuronlist[2], neuronlist[3]) / np.sqrt(neuronlist[2]/2)\n",
    "    b1 = np.zeros(neuronlist[1])\n",
    "    b2 = np.zeros(neuronlist[2])\n",
    "    b3 = np.zeros(neuronlist[3])\n",
    "        \n",
    "    return W1, W2, W3, b1, b2, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearLayer:\n",
    "    '''\n",
    "    이 클래스의 인스턴스는 ThreeLayerNet 클래스의 __init__() 메서드에서 만들어진다.\n",
    "    '''\n",
    "    def __init__(self, W, b):\n",
    "        #backward에 필요한 X, W, b 값 저장 + dW, db값 받아오기\n",
    "        \n",
    "        self.X = None\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.X = x\n",
    "        #내적연산을 통한 Z값 계산\n",
    "        Z = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        #백워드 함수\n",
    "        '''\n",
    "        dx의 크기는 [데이터 개수, 뉴런 개수]x[뉴런 개수, 784] = [데이터 개수, 784]\n",
    "        dW의 크기는 [784, 데이터 개수]x[데이터 개수, 뉴런 개수] = [784, 뉴런 개수]\n",
    "        db의 크기는 [뉴런 개수, ] 이다.\n",
    "        '''\n",
    "        dx = np.dot(dZ, self.W.T)\n",
    "        self.dW = np.dot(self.X.T, dZ)\n",
    "        self.db = np.sum(dout, axis = 0)        \n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiLU:\n",
    "    '''\n",
    "    SiLU란 A = z ∗ sigmoid(z)로 나타나는 그래프이다.\n",
    "    forward함수에서는, z라는 입력이 들어오면 SiLU를 activation function으로 하여 activate한 후 그 결과를 self.Z에 저장한다.\n",
    "    backward함수에서는, 저장한 Z값으로 SiLU의 미분값을 구한 후 앞의 레이어에서 backward로 들어온 dActivation 값을 곱한 값 dZ를 출력한다.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = None # 백워드 시 사용할 로컬 변수\n",
    "       \n",
    "    \n",
    "    def forward(self, Z):\n",
    "        #수식에 따른 forward 함수 작성\n",
    "        self.Z = Z\n",
    "        Activation = Z * (1 / (1 + np.exp(-Z)))\n",
    "\n",
    "        return Activation\n",
    "    \n",
    "    \n",
    "    def backward(self, dActivation):\n",
    "        '''\n",
    "        연산 과정을 도식화하면 아래와 같다.\n",
    "          Z                         sig(Z)          Activation\n",
    "        ----------> (Sigmoid) ------------> (*) ----------------> \n",
    "          dZ                         ds      ↑    dActivation\n",
    "                                             │ \n",
    "                                 Z ----------┘\n",
    "        이 때 ds = dActivation * Z 이고, dZ = ds * (sig(Z)*(1-sig(Z))) 이다.\n",
    "        '''\n",
    "        sig = 1 / (1 + np.exp(-self.Z))\n",
    "        ds = self.Z * dActivation\n",
    "        dZ = ds * (sig * (1 - sig))\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss(): # 제공\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.softmaxScore = None\n",
    "        self.label = None\n",
    "        \n",
    "    def forward(self, score, one_hot_label):\n",
    "        \n",
    "        batch_size = one_hot_label.shape[0]\n",
    "        self.label = one_hot_label\n",
    "        self.softmaxScore = Softmax(score)\n",
    "        self.loss = -np.sum(self.label * np.log(self.softmaxScore + 1e-20)) / batch_size\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.label.shape[0]\n",
    "        dx = (self.softmaxScore - self.label) / batch_size\n",
    "        \n",
    "        return dx\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNet :\n",
    "    \n",
    "    def __init__(self, paramlist):\n",
    "        \n",
    "        W1, W2, W3, b1, b2, b3 = setParam_He(paramlist)\n",
    "        self.params = {}\n",
    "        self.params['W1'] = W1\n",
    "        self.params['W2'] = W2\n",
    "        self.params['W3'] = W3\n",
    "        self.params['b1'] = b1\n",
    "        self.params['b2'] = b2\n",
    "        self.params['b3'] = b3\n",
    "        \n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        self.layers['L1'] = linearLayer(self.params['W1'], self.params['b1'])\n",
    "        self.layers['SiLU1'] = SiLU()\n",
    "        self.layers['L2'] = linearLayer(self.params['W2'], self.params['b2'])\n",
    "        self.layers['SiLU2'] = SiLU()\n",
    "        self.layers['L3'] = linearLayer(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def scoreFunction(self, x):\n",
    "        \n",
    "        for layer in self.layers.values():\n",
    "            # 한 줄이 best\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        score = x\n",
    "        return score\n",
    "        \n",
    "    def forward(self, x, label):\n",
    "        \n",
    "        score = self.scoreFunction(x)\n",
    "        return self.lastLayer.forward(score, label)\n",
    "    \n",
    "    def accuracy(self, x, label):\n",
    "        \n",
    "        score = self.scoreFunction(x)\n",
    "        score_argmax = np.argmax(score, axis=1)\n",
    "        \n",
    "        if label.ndim != 1 : #label이 one_hot_encoding 된 데이터면 if문을 \n",
    "            label_argmax = np.argmax(label, axis = 1)\n",
    "            \n",
    "        accuracy  = np.sum(score_argmax==label_argmax) / int(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def backpropagation(self, x, label):\n",
    "        \n",
    "        #백워드 함수 작성 스코어펑션을 참고하세요\n",
    "        dL = lastLayer.backward()\n",
    "        dA2 = layers['L3'].backward(dL)\n",
    "        dZ2 = layers['SiLU2'].backward(dA2)\n",
    "        dA1 = layers['L2'].backward(dZ2)\n",
    "        dZ1 = layers['SiLU1'].backward(dA1)\n",
    "        d = layers['L1'].backward(dZ1)\n",
    "            \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['L1'].dW\n",
    "        grads['b1'] = self.layers['L1'].db\n",
    "        grads['W2'] = self.layers['L2'].dW\n",
    "        grads['b2'] = self.layers['L2'].db\n",
    "        grads['W3'] = self.layers['L3'].dW\n",
    "        grads['b3'] = self.layers['L3'].db\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradientdescent(self, grads, learning_rate):\n",
    "        \n",
    "        self.params['W1'] -= learning_rate*grads['W1']\n",
    "        self.params['W2'] -= learning_rate*grads['W2']\n",
    "        self.params['W3'] -= learning_rate*grads['W3']\n",
    "        self.params['b1'] -= learning_rate*grads['b1']\n",
    "        self.params['b2'] -= learning_rate*grads['b2']\n",
    "        self.params['b3'] -= learning_rate*grads['b3']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchOptimization(dataset, ThreeLayerNet, learning_rate, epoch=1000):\n",
    "    \n",
    "    for i in range(epoch+1):\n",
    "        #코드 작성\n",
    "        Loss = ThreeLayerNet.forward(dataset['train_X'], dataset['one_hot_train'])\n",
    "        ThreeLayerNet.gradientdescent(ThreeLayaerNet.backpropagation(dataset['train_X'], dataset['one_hot_train']), learning_late)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            train_acc = ThreeLayerNet.accuracy(dataset['train_X'], dataset['one_hot_train'])\n",
    "            test_acc = ThreeLayerNet.accuracy(dataset['test_X'], dataset['one_hot_test'])\n",
    "            print(i, '\\t번째 Loss = ', Loss)\n",
    "            print(i, '\\t번째 Train_Accuracy : ', train_acc)\n",
    "            print(i, '\\t번째 Test_Accuracy : ', test_acc)\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            Loss_list.append(Loss)        \n",
    "   \n",
    "    return ThreeLayerNet, train_acc_list, test_acc_list, Loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_Optimization(dataset, ThreeLayerNet, learning_rate, epoch=100, batch_size=1000):    \n",
    "    \n",
    "    np.random.seed(5)\n",
    "    for i in range(epoch+1):\n",
    "        # 코드 작성\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            train_acc = ThreeLayerNet.accuracy(dataset['train_X'], dataset['one_hot_train'])\n",
    "            test_acc = ThreeLayerNet.accuracy(dataset['test_X'], dataset['one_hot_test'])\n",
    "            print(i, '\\t번째 Loss = ', Loss)\n",
    "            print(i, '\\t번째 Train_Accuracy : ', train_acc)\n",
    "            print(i, '\\t번째 Test_Accuracy : ', test_acc)\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            Loss_list.append(Loss)  \n",
    "\n",
    "    return ThreeLayerNet, train_acc_list, test_acc_list, Loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_use_Optimizer(dataset, ThreeLayerNet, learning_rate, epoch, kill_n_h1 = 0.25, kill_n_h2 = 0.15):\n",
    "    \n",
    "    for i in range(epoch+1):\n",
    "        #코드 작성\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            train_acc = ThreeLayerNet.accuracy(dataset['train_X'], dataset['one_hot_train'])\n",
    "            test_acc = ThreeLayerNet.accuracy(dataset['test_X'], dataset['one_hot_test'])\n",
    "            print(i, '\\t번째 Loss = ', Loss)\n",
    "            print(i, '\\t번째 Train_Accuracy : ', train_acc)\n",
    "            print(i, '\\t번째 Test_Accuracy : ', test_acc)\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            Loss_list.append(Loss)  \n",
    "    return ThreeLayerNet, train_acc_list, test_acc_list, Loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#과제 채점을 위한 세팅\n",
    "train_X, train_label, test_X, test_label = train_test_split(mnist)\n",
    "\n",
    "one_hot_train = one_hot_encoding(train_label)\n",
    "one_hot_test = one_hot_encoding(test_label)\n",
    "\n",
    "dataset = {}\n",
    "dataset['train_X'] = train_X\n",
    "dataset['test_X'] = test_X\n",
    "dataset['one_hot_train'] = one_hot_train\n",
    "dataset['one_hot_test'] = one_hot_test\n",
    "\n",
    "neournlist = [784, 60, 30, 10]\n",
    "\n",
    "TNN_batchOptimizer = ThreeLayerNet(neournlist)\n",
    "TNN_minibatchOptimizer = copy.deepcopy(TNN_batchOptimizer)\n",
    "TNN_dropout = copy.deepcopy(TNN_minibatchOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#채점은 이 것의 결과값으로 할 예정입니다. \n",
    "\n",
    "trained_batch, tb_train_acc_list, tb_test_acc_list, tb_loss_list =  batchOptimization(dataset, TNN_batchOptimizer, 0.1, 500)\n",
    "trained_minibatch, tmb_train_acc_list, tmb_test_acc_list, tb_loss_list = minibatch_Optimization(dataset, TNN_minibatchOptimizer, 0.01, epoch=100, batch_size=1000)\n",
    "trained_dropout, td_train_acc_list, td_test_acc_list, td_loss_list = dropout_use_Optimizer(dataset, TNN_dropOut, learning_rate, 1000, 0.25, 0.15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
