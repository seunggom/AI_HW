{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이 코드는 리눅스 제출용입니다. 보고서에는 처음 제공해준 2차과제로 작성해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = np.loadtxt('mnist.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(csv_dataset): # i는 0~100 중의 하나의 수로 train_set의 비율을 나타낸다. ex) 70 => train_set 70% test_set 30%\n",
    "    #코드 작성\n",
    "    '''\n",
    "    csv_dataset의 shape는 (10000, 785)이다.\n",
    "    총 만 개의 데이터가 있고, 각 데이터는 레이블값(1개), 픽셀값(784개)들로 이루어져 있다.\n",
    "    train_set 개수 : test_set 개수 = 80 : 20 의 비율로 데이터를 분할하고 레이블값과 픽셀값으로 한 번 더 분할하면,\n",
    "    train_X.shape = (8000, 784)\n",
    "    train_T.shape = (8000, 1)\n",
    "    test_X.shape = (2000, 784)\n",
    "    test_T.shape = (2000, 1) 이다.\n",
    "    '''\n",
    "    train_X = csv_dataset[:8000, 1:]\n",
    "    train_X /= 256\n",
    "    train_T = csv_dataset[:8000, 0]\n",
    "    test_X = csv_dataset[8000:, 1:]\n",
    "    test_X /= 256\n",
    "    test_T = csv_dataset[8000:, 0]\n",
    "    \n",
    "    return train_X, train_T, test_X, test_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(T): # T is data의 label\n",
    "    #코드 작성\n",
    "    one_hot_label = np.zeros([T.shape[0],10])\n",
    "    T = T.astype(np.uint8)\n",
    "    one_hot_label[np.arange(T.shape[0]), T] = 1\n",
    "    \n",
    "    '''\n",
    "    먼저 [데이터개수, 10] 크기의 배열을 만든다\n",
    "    각 데이터마다의 레이블값과 같은 인덱스열에 1의 값을 넣어주어야 하는데,\n",
    "    T에 들어있는 값은 float형이기 때문에, 형변환을 하지 않고 3번째 줄을 실행하면\n",
    "    IndexError: arrays used as indices must be of integer (or boolean) type 와 같은 에러가 발생한다.\n",
    "    그래서 np.astype을 이용해 int형으로 변환해주었다.\n",
    "    그리고 one_hot_label에서 각 행마다 정답 인덱스에 해당하는 열에 1의 값을 저장한다.\n",
    "    \n",
    "    이 함수의 예를 들자면, T=[7,2]일 때 one_hot_label은\n",
    "    [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
    "     [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] 가 나오게 된다.\n",
    "    '''\n",
    "    \n",
    "    return one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(ScoreMatrix): # 제공.\n",
    "\n",
    "    if ScoreMatrix.ndim == 2:\n",
    "        temp = ScoreMatrix\n",
    "        temp = temp - np.max(temp, axis=1, keepdims=True)\n",
    "        y_predict = np.exp(temp) / np.sum(np.exp(temp), axis=1, keepdims=True)\n",
    "        return y_predict\n",
    "    temp = ScoreMatrix - np.max(ScoreMatrix, axis=0)\n",
    "    expX = np.exp(temp)\n",
    "    y_predict = expX / np.sum(expX)\n",
    "    return y_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setParam_He(neuronlist):\n",
    "    \n",
    "    np.random.seed(1) # seed값 고정을 통해 input이 같으면 언제나 같은 Weight와 bias를 출력하기 위한 함수\n",
    "    \n",
    "    '''\n",
    "    input layer neuron, hidden layer1 neuron, hidden layer2 neuron과 연산을 할 각각의 Weight가 필요하다.\n",
    "    이 Weight값과 Bias 값을 He의 방법으로 초기화를 한다.\n",
    "    He의 방법을 이용한 Weight의 초기값이란 앞 계층의 노드가 n 개일 때, 표준편차가 (2/n)^0.5 인 정규분포를 사용하는 것을 말한다.\n",
    "    input layer neuron의 크기는 [데이터 개수, 784],\n",
    "    hidden layer1 neuron의 크기는 [데이터 개수, 60],\n",
    "    hidden layer2 neuron의 크기는 [데이터 개수, 30],\n",
    "    output layer neuron의 크기는 [데이터 개수, 10] 이므로\n",
    "    필요한 Weight의 크기는 순서대로 [784, 60], [60, 30], [30, 10]이 될 것이다.\n",
    "    bias의 경우에는 초기값으로 전부 0의 값을 제공한다.\n",
    "    bias의 크기는 순서대로 [60, ], [30, ], [10, ] 이다.\n",
    "    '''\n",
    "    \n",
    "    W1 = np.random.randn(neuronlist[0], neuronlist[1]) / np.sqrt(neuronlist[0]/2)\n",
    "    W2 = np.random.randn(neuronlist[1], neuronlist[2]) / np.sqrt(neuronlist[1]/2)\n",
    "    W3 = np.random.randn(neuronlist[2], neuronlist[3]) / np.sqrt(neuronlist[2]/2)\n",
    "    b1 = np.zeros(neuronlist[1])\n",
    "    b2 = np.zeros(neuronlist[2])\n",
    "    b3 = np.zeros(neuronlist[3])\n",
    "    \n",
    "    return W1, W2, W3, b1, b2, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearLayer:\n",
    "    '''\n",
    "    이 클래스의 인스턴스는 ThreeLayerNet 클래스의 __init__() 메서드에서 만들어진다.\n",
    "    각각의 레이어에 필요한 forward, backward 연산을 하는 함수를 제공한다.\n",
    "    '''\n",
    "    def __init__(self, W, b):\n",
    "        #backward에 필요한 X, W, b 값 저장 + dW, db값 받아오기\n",
    "        \n",
    "        self.X = None\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        내적연산을 통한 Z값 계산하는 함수이다.\n",
    "        Z가 의미하는 것은 이전 레이어의 뉴런들에 weight만큼의 가중치를 적용한 신호의 총합들이다.\n",
    "        즉 값이 높은 뉴런일 수록 그것이 정답일 가능성이 높다고 추측한 것이다.\n",
    "        '''\n",
    "        self.X = x\n",
    "        Z = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        #백워드 함수\n",
    "        '''\n",
    "        gradient를 계산하는 함수이다.\n",
    "        dx의 크기는 [데이터 개수, 뉴런 개수]x[뉴런 개수, 784] = [데이터 개수, 784]\n",
    "        dW의 크기는 [784, 데이터 개수]x[데이터 개수, 뉴런 개수] = [784, 뉴런 개수]\n",
    "        db의 크기는 [뉴런 개수, ] 이므로,\n",
    "        크기에 맞게 내적연산을 진행한다.\n",
    "        '''\n",
    "        dx = np.dot(dZ, self.W.T)\n",
    "        self.dW = np.dot(self.X.T, dZ)\n",
    "        self.db = np.sum(dZ, axis = 0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    '''\n",
    "    드랍아웃을 위한 클래스이다.\n",
    "    드랍아웃을 사용할 경우 이 클래스의 객체가 생성되고, 각 히든 레이어마다 kill_rate만큼 값을 죽인다.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.kill_rate = None\n",
    "        self.U = None\n",
    "\n",
    "    def forward(self, D, train_flag=True):\n",
    "        '''\n",
    "        드랍아웃으로 뉴런을 죽이는 것은 training 과정에서만 진행해야하므로\n",
    "        forward는 train_flag로 training과 testing 과정을 구분하여 진행한다.\n",
    "        U = np.random.rand(*D.shape) >= kill_rate 의 의미를 설명하자면,\n",
    "        np.random.rand(*D.shape)은 D의 크기만큼 0에서 1사이의 값을 가지는 행렬을 리턴한다.\n",
    "        이 때 kill_rate 이상인 값을 가지면 식이 참이므로 1의 값을 가지고 kill_rate 미만의 값을 가지면 식이 거짓이므로 0을 가진다.\n",
    "        그래서 0 혹은 1의 값을 가지고 있는 U을 x에 곱하면, kill_rate의 확률만큼 뉴런이 죽게된다.\n",
    "        \n",
    "        testing 과정에서는 모든 뉴런을 사용하기 때문에\n",
    "        training 과정에서의 출력 뉴런 데이터의 기대값과 동일한 기대값을 갖기 위해서는 (1 - kill_rate) 만큼 곱해주어야 한다.\n",
    "        '''\n",
    "        if train_flag:\n",
    "            self.U = np.random.rand(*D.shape) >= self.kill_rate\n",
    "            return D * self.U\n",
    "        else:\n",
    "            return D * (1 - self.kill_rate)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        '''\n",
    "        forward 과정에서 죽였던 뉴런 위치 그대로 backward를 진행한다.\n",
    "        '''\n",
    "        dx = dout * self.U\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiLU:\n",
    "    '''\n",
    "    SiLU란 f(z) = z ∗ sigmoid(z)로 나타나는 함수이다.\n",
    "    forward함수에서는, z라는 입력이 들어오면 SiLU를 activation function으로 하여 activate한 후 그 결과를 self.Z에 저장한다.\n",
    "    backward함수에서는, 저장한 Z값으로 SiLU의 미분값을 구한 후 앞의 레이어에서 backward로 들어온 dActivation 값을 곱한 값 dZ를 출력한다.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = None # 백워드 시 사용할 로컬 변수\n",
    "       \n",
    "    \n",
    "    def forward(self, Z):\n",
    "        #수식에 따른 forward 함수 작성\n",
    "        sig = 1 / (1 + np.exp(-Z))\n",
    "        Activation = Z * sig\n",
    "        self.Z = Z\n",
    "\n",
    "        return Activation\n",
    "    \n",
    "    \n",
    "    def backward(self, dActivation):\n",
    "        '''\n",
    "        연산 과정을 도식화하면 아래와 같다.\n",
    "          Z                    Activation\n",
    "        ----------> (SiLU) ---------------->\n",
    "          dZ                 dActivation\n",
    "\n",
    "        이 때, f'(z) = f(z) + sigmoid(z)(1-f(z)) 이므로\n",
    "        dZ = (f(z) + sigmoid(z)(1-f(z))) * dActivation 이다.\n",
    "        '''\n",
    "        sig = 1 / (1 + np.exp(-self.Z))\n",
    "        fz = self.forward(self.Z)\n",
    "        dZ = (sig * (1 - fz) + fz) * dActivation\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss(): # 제공\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.softmaxScore = None\n",
    "        self.label = None\n",
    "        \n",
    "    def forward(self, score, one_hot_label):\n",
    "        \n",
    "        batch_size = one_hot_label.shape[0]\n",
    "        self.label = one_hot_label\n",
    "        self.softmaxScore = Softmax(score)\n",
    "        self.loss = -np.sum(self.label * np.log(self.softmaxScore + 1e-20)) / batch_size\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.label.shape[0]\n",
    "        dx = (self.softmaxScore - self.label) / batch_size\n",
    "        \n",
    "        return dx\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNet :\n",
    "    \n",
    "    def __init__(self, paramlist):\n",
    "        \n",
    "        W1, W2, W3, b1, b2, b3 = setParam_He(paramlist)\n",
    "        self.params = {}\n",
    "        self.params['W1'] = W1\n",
    "        self.params['W2'] = W2\n",
    "        self.params['W3'] = W3\n",
    "        self.params['b1'] = b1\n",
    "        self.params['b2'] = b2\n",
    "        self.params['b3'] = b3\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        self.layers['L1'] = linearLayer(self.params['W1'], self.params['b1'])\n",
    "        self.layers['SiLU1'] = SiLU()\n",
    "        self.layers['L2'] = linearLayer(self.params['W2'], self.params['b2'])\n",
    "        self.layers['SiLU2'] = SiLU()\n",
    "        self.layers['L3'] = linearLayer(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def scoreFunction(self, x):\n",
    "        '''\n",
    "        모든 레이어에 대해 차례대로 forward를 진행한다.\n",
    "        그리고 리턴값으로는 forward를 모두 마친 후의 score값을 리턴한다.\n",
    "        '''\n",
    "        for layer in self.layers.values():\n",
    "            # 한 줄이 best\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        score = x\n",
    "        return score\n",
    "        \n",
    "    def forward(self, x, label):\n",
    "        '''\n",
    "        위의 scoreFunction 함수를 이용해 score를 구하고,\n",
    "        loss를 구하여 리턴하는 함수이다.\n",
    "        '''\n",
    "        score = self.scoreFunction(x)\n",
    "        return self.lastLayer.forward(score, label)\n",
    "    \n",
    "    def accuracy(self, x, label):\n",
    "        \n",
    "        score = self.scoreFunction(x)\n",
    "        score_argmax = np.argmax(score, axis=1)\n",
    "        \n",
    "        if label.ndim != 1 : #label이 one_hot_encoding 된 데이터면 if문을 \n",
    "            label_argmax = np.argmax(label, axis = 1)\n",
    "            \n",
    "        accuracy = np.sum(score_argmax==label_argmax) / int(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def backpropagation(self):\n",
    "        '''\n",
    "        forward를 진행한 레이어 순서의 반대 순서로 backward를 진행한다.\n",
    "        backward 후, 각 레이어 내 저장되어 있는 dW, db값을 grads 라는 딕셔너리 객체에 저장한 후 그 값을 리턴한다.\n",
    "        \n",
    "        '''\n",
    "        dL = self.lastLayer.backward()\n",
    "        dA2 = self.layers['L3'].backward(dL)\n",
    "        dZ2 = self.layers['SiLU2'].backward(dA2)\n",
    "        dA1 = self.layers['L2'].backward(dZ2)\n",
    "        dZ1 = self.layers['SiLU1'].backward(dA1)\n",
    "        d = self.layers['L1'].backward(dZ1)\n",
    "            \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['L1'].dW\n",
    "        grads['b1'] = self.layers['L1'].db\n",
    "        grads['W2'] = self.layers['L2'].dW\n",
    "        grads['b2'] = self.layers['L2'].db\n",
    "        grads['W3'] = self.layers['L3'].dW\n",
    "        grads['b3'] = self.layers['L3'].db\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradientdescent(self, grads, learning_rate):\n",
    "        \n",
    "        self.params['W1'] -= learning_rate*grads['W1']\n",
    "        self.params['W2'] -= learning_rate*grads['W2']\n",
    "        self.params['W3'] -= learning_rate*grads['W3']\n",
    "        self.params['b1'] -= learning_rate*grads['b1']\n",
    "        self.params['b2'] -= learning_rate*grads['b2']\n",
    "        self.params['b3'] -= learning_rate*grads['b3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UseDropout_ThreeLayerNet:\n",
    "    '''\n",
    "    처음엔 기존의 ThreeLayerNet 클래스를 수정하여 드랍아웃 구현을 하려고 했는데,\n",
    "    구현할수록 코드가 복잡해져서 드랍아웃 과정이 추가된 ThreeLayerNet 클래스를 따로 구현하였다.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, paramlist):\n",
    "        \n",
    "        W1, W2, W3, b1, b2, b3 = setParam_He(paramlist)\n",
    "        self.params = {}\n",
    "        self.params['W1'] = W1\n",
    "        self.params['W2'] = W2\n",
    "        self.params['W3'] = W3\n",
    "        self.params['b1'] = b1\n",
    "        self.params['b2'] = b2\n",
    "        self.params['b3'] = b3\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    " \n",
    "        self.layers['L1'] = linearLayer(self.params['W1'], self.params['b1'])\n",
    "        self.layers['SiLU1'] = SiLU()\n",
    "        self.layers['L2'] = linearLayer(self.params['W2'], self.params['b2'])\n",
    "        self.layers['SiLU2'] = SiLU()\n",
    "        self.layers['L3'] = linearLayer(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        #드랍아웃을 위한 레이어 두개\n",
    "        self.dropout_layer1 = Dropout()\n",
    "        self.dropout_layer2 = Dropout()\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "        \n",
    "    def scoreFunction(self, x, train_flag):\n",
    "        '''\n",
    "        레이어 순서대로 forward를 진행하는데, 뉴런값들을 activate한 후에 train_flag에 따라서 dropout을 진행한다.\n",
    "        '''\n",
    "        x = self.layers['L1'].forward(x)\n",
    "        x = self.layers['SiLU1'].forward(x)\n",
    "        u1 = self.dropout_layer1.forward(x, train_flag)\n",
    "        x = self.layers['L2'].forward(u1)\n",
    "        x = self.layers['SiLU2'].forward(x)\n",
    "        u2 = self.dropout_layer2.forward(x, train_flag)\n",
    "        x = self.layers['L3'].forward(u2)\n",
    "        \n",
    "        score = x\n",
    "        return score\n",
    "        \n",
    "    def forward(self, x, label, train_flag=True):\n",
    "        '''\n",
    "        score를 계산하고, 그것에 따라 Loss를 리턴한다.\n",
    "        '''\n",
    "        score = self.scoreFunction(x, train_flag)\n",
    "\n",
    "        return self.lastLayer.forward(score, label)\n",
    "    \n",
    "    def accuracy(self, x, label):\n",
    "        '''\n",
    "        정확도를 구하는 함수이다.\n",
    "        이 때는 오로지 각 데이터셋에 대한 정확도를 구하면 되기 때문에 train_flag를 false로 설정하여 뉴런을 죽이는 과정은 하지 않는다.\n",
    "        '''\n",
    "        train_flag = False\n",
    "        score = self.scoreFunction(x, train_flag)\n",
    "        score_argmax = np.argmax(score, axis=1)\n",
    "        \n",
    "        if label.ndim != 1 : #label이 one_hot_encoding 된 데이터면 if문을 \n",
    "            label_argmax = np.argmax(label, axis = 1)\n",
    "            \n",
    "        accuracy = np.sum(score_argmax==label_argmax) / int(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def backward(self):\n",
    "        '''\n",
    "        dropout 레이어의 forward에서 지정한 확률만큼 뉴런을 죽인것처럼, backward에서도 똑같이 gradient값을 죽인다.\n",
    "        forward에서 진행한 레이어 순서의 반대로 진행한다.\n",
    "        '''\n",
    "        dL = self.lastLayer.backward()\n",
    "        dA2 = self.layers['L3'].backward(dL)\n",
    "        dD2 = self.dropout_layer2.backward(dA2)\n",
    "        dZ2 = self.layers['SiLU2'].backward(dD2)\n",
    "        dA1 = self.layers['L2'].backward(dZ2)\n",
    "        dD1 = self.dropout_layer1.backward(dA1)\n",
    "        dZ1 = self.layers['SiLU1'].backward(dD1)\n",
    "        d = self.layers['L1'].backward(dZ1)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['L1'].dW\n",
    "        grads['b1'] = self.layers['L1'].db\n",
    "        grads['W2'] = self.layers['L2'].dW\n",
    "        grads['b2'] = self.layers['L2'].db\n",
    "        grads['W3'] = self.layers['L3'].dW\n",
    "        grads['b3'] = self.layers['L3'].db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def gradientdescent(self, grads, learning_rate):\n",
    "        \n",
    "        self.params['W1'] -= learning_rate*grads['W1']\n",
    "        self.params['W2'] -= learning_rate*grads['W2']\n",
    "        self.params['W3'] -= learning_rate*grads['W3']\n",
    "        self.params['b1'] -= learning_rate*grads['b1']\n",
    "        self.params['b2'] -= learning_rate*grads['b2']\n",
    "        self.params['b3'] -= learning_rate*grads['b3']\n",
    "        \n",
    "        \n",
    "    def setKillRate(self, r1, r2):\n",
    "        '''\n",
    "        각 히든레이어에서 쓸 kill_rate의 값을 받아서 업데이트한다.\n",
    "        '''\n",
    "        self.dropout_layer1.kill_rate = r1\n",
    "        self.dropout_layer2.kill_rate = r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchOptimization(dataset, ThreeLayerNet, learning_rate=0.1, epoch=300):\n",
    "    \n",
    "    for i in range(epoch+1):\n",
    "        #코드 작성\n",
    "        Loss = ThreeLayerNet.forward(dataset['train_X'], dataset['one_hot_train'])\n",
    "        grad = ThreeLayerNet.backpropagation()\n",
    "        ThreeLayerNet.gradientdescent(grad, learning_rate)\n",
    "        \n",
    "\n",
    "    \n",
    "    #epoch을 다 실행시킨 후 트레이닝이 완료된 후의 Acc와 Loss 출력\n",
    "    train_acc = ThreeLayerNet.accuracy(dataset['train_X'], dataset['one_hot_train'])\n",
    "    test_acc = ThreeLayerNet.accuracy(dataset['test_X'], dataset['one_hot_test'])\n",
    "    print('트레이닝 후 batch Loss = ', Loss)\n",
    "    print('트레이닝 후 batch Train_Accuracy : ', train_acc)\n",
    "    print('트레이닝 후 batch Test_Accuracy : ', test_acc)\n",
    "    \n",
    "   \n",
    "    return ThreeLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_Optimization(dataset, ThreeLayerNet, learning_rate = 0.1, epoch=50, batch_size=100):    \n",
    "    \n",
    "    np.random.seed(5)\n",
    "    for i in range(epoch+1):\n",
    "        # 코드 작성\n",
    "        tmp_train = np.concatenate((dataset['one_hot_train'], dataset['train_X']), axis=1)\n",
    "        np.random.shuffle(tmp_train)\n",
    "        dataset['one_hot_train'] = tmp_train[:, :10]\n",
    "        dataset['train_X'] = tmp_train[:, 10:]\n",
    "\n",
    "        batch = {}\n",
    "        b = 0\n",
    "        '''\n",
    "        미니배치 크기가 100이고 데이터의 갯수가 8000개인 경우\n",
    "        위에서 랜덤하게 섞은 데이터를 매 반복문마다 슬라이싱을 이용해\n",
    "        0~99번째, 100~199번째, ..., 7900~7999번째 데이터로 나누어서 연산을 진행한다.\n",
    "        '''\n",
    "        while b < (tmp_train.shape[0] / batch_size):\n",
    "            batch['train_X'] = dataset['train_X'][(batch_size * b):(batch_size * (b+1)), :]\n",
    "            batch['one_hot_train'] = dataset['one_hot_train'][(batch_size * b):(batch_size * (b+1)), :]\n",
    "            \n",
    "            Loss = ThreeLayerNet.forward(batch['train_X'], batch['one_hot_train'])\n",
    "            grad = ThreeLayerNet.backpropagation()\n",
    "            ThreeLayerNet.gradientdescent(grad, learning_rate)\n",
    "            # 각각의 미니배치마다 loss, gradient를 구해서 weight와 bias를 업데이트 한다.\n",
    "            b += 1\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    #epoch을 다 실행시킨 후 트레이닝이 완료된 후의 Acc와 Loss 출력    \n",
    "    train_acc = ThreeLayerNet.accuracy(dataset['train_X'], dataset['one_hot_train'])\n",
    "    test_acc = ThreeLayerNet.accuracy(dataset['test_X'], dataset['one_hot_test'])\n",
    "    print('트레이닝 후 minibatch Loss = ', Loss)\n",
    "    print('트레이닝 후 minibatch Train_Accuracy : ', train_acc)\n",
    "    print('트레이닝 후 minibatch Test_Accuracy : ', test_acc)\n",
    "\n",
    "    return ThreeLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_use_Optimizer(dataset, UseDropout_ThreeLayerNet, learning_rate =0.1, epoch=300, kill_n_h1 = 0.25, kill_n_h2 = 0.15):\n",
    "    '''\n",
    "    Dropout을 사용한 Optimization이다.\n",
    "    Dropout을 사용하는 이유는 train data에만 치중하여 weight를 업데이트하면\n",
    "    오히려 test data의 정확도가 떨어질 수 있어 이를 방지하기 위함이다.\n",
    "    kill_n_h1, kill_n_h2은 드롭아웃으로 뉴런을 죽이는 비율이다.\n",
    "    UseDropout_ThreeLayerNet 클래스를 이용하였다.\n",
    "    '''\n",
    "    \n",
    "    UseDropout_ThreeLayerNet.setKillRate(kill_n_h1, kill_n_h2) # 뉴런을 죽일 비율을 dropout 레이어에 전달한다.\n",
    "    \n",
    "    for i in range(epoch+1):\n",
    "        #코드 작성\n",
    "        Loss = UseDropout_ThreeLayerNet.forward(dataset['train_X'], dataset['one_hot_train'])\n",
    "        grad = UseDropout_ThreeLayerNet.backward()\n",
    "        UseDropout_ThreeLayerNet.gradientdescent(grad, learning_rate)\n",
    "\n",
    "    \n",
    "    \n",
    "    #epoch을 다 실행시킨 후 트레이닝이 완료된 후의 Acc와 Loss 출력    \n",
    "    train_acc = UseDropout_ThreeLayerNet.accuracy(dataset['train_X'], dataset['one_hot_train'])\n",
    "    test_acc = UseDropout_ThreeLayerNet.accuracy(dataset['test_X'], dataset['one_hot_test'])\n",
    "    print('트레이닝 후 dropout Loss = ', Loss)\n",
    "    print('트레이닝 후 dropout Train_Accuracy : ', train_acc)\n",
    "    print('트레이닝 후 dropout Test_Accuracy : ', test_acc)\n",
    "  \n",
    "    return UseDropout_ThreeLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#과제 채점을 위한 세팅\n",
    "train_X, train_label, test_X, test_label = train_test_split(mnist)\n",
    "\n",
    "one_hot_train = one_hot_encoding(train_label)\n",
    "one_hot_test = one_hot_encoding(test_label)\n",
    "\n",
    "dataset = {}\n",
    "dataset['train_X'] = train_X\n",
    "dataset['test_X'] = test_X\n",
    "dataset['one_hot_train'] = one_hot_train\n",
    "dataset['one_hot_test'] = one_hot_test\n",
    "\n",
    "neournlist = [784, 60, 30, 10]\n",
    "\n",
    "TNN_batchOptimizer = ThreeLayerNet(neournlist)\n",
    "TNN_minibatchOptimizer = copy.deepcopy(TNN_batchOptimizer)\n",
    "TNN_dropOut = UseDropout_ThreeLayerNet(neournlist) # UseDropout_ThreeLayerNet 클래스 객체 생성함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트레이닝 후 batch Loss =  0.2521006777530326\n",
      "트레이닝 후 batch Train_Accuracy :  0.928375\n",
      "트레이닝 후 batch Test_Accuracy :  0.932\n",
      "트레이닝 후 minibatch Loss =  0.00589678860174121\n",
      "트레이닝 후 minibatch Train_Accuracy :  1.0\n",
      "트레이닝 후 minibatch Test_Accuracy :  0.9575\n",
      "트레이닝 후 dropout Loss =  0.53915460277771\n",
      "트레이닝 후 dropout Train_Accuracy :  0.897125\n",
      "트레이닝 후 dropout Test_Accuracy :  0.9205\n"
     ]
    }
   ],
   "source": [
    "#채점은 이 것의 결과값으로 할 예정입니다. \n",
    "\n",
    "trained_batch =  batchOptimization(dataset, TNN_batchOptimizer, learning_rate = 0.1, epoch=300)\n",
    "trained_minibatch = minibatch_Optimization(dataset, TNN_minibatchOptimizer, learning_rate = 0.1, epoch=50, batch_size=100)\n",
    "trained_dropout = dropout_use_Optimizer(dataset, TNN_dropOut, learning_rate=0.1, epoch =300, kill_n_h1 = 0.25, kill_n_h2 = 0.15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
